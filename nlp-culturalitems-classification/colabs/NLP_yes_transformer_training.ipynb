{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNPyhPqqQmv-"
   },
   "source": [
    "# NLP Transformer: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAbYmknhuTBe",
    "outputId": "24a74266-a207-4e5a-e8ae-69f564517281"
   },
   "outputs": [],
   "source": [
    "!pip install Wikidata\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25oLl6jmtrlr"
   },
   "outputs": [],
   "source": [
    "import torch, random, requests, os, pickle\n",
    "import numpy as np\n",
    "from wikidata.client import Client\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from google.colab import userdata\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuBPpIx-kuB2"
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQoa3pVfRZ_-"
   },
   "source": [
    "Setting seeds to try to make the training as much deterministic as possible.\n",
    "This should help to compare results (for the instance accuracy of the validation test) of different trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUCscD1MddhE"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Q8SGqmNkxOq"
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOP1h3JWRe4P"
   },
   "source": [
    "Wikipedia pages and Wikidata data need to be loaded from the web.\n",
    "In order to speed up the training and the inference processes, we store (cache) the retrieved and the processed data into .bin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCas4n7JePkX"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "BASE_PATH = '/content/drive/MyDrive/Lost_in_Language_Recognition/'\n",
    "\n",
    "def dump(file_name, result):\n",
    "    file_path = BASE_PATH + file_name\n",
    "    # remove dump files if present\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        print(\"dumping\", file_path)\n",
    "        # noinspection PyTypeChecker\n",
    "        pickle.dump(result, file)\n",
    "\n",
    "def load(file_name):\n",
    "    file_path = BASE_PATH + file_name\n",
    "    with open(file_path, 'rb') as file:\n",
    "        print(\"loading\", file_path)\n",
    "        # noinspection PyTypeChecker\n",
    "        return pickle.load(file)\n",
    "\n",
    "def file_exists(file_name):\n",
    "    file_path = BASE_PATH + file_name\n",
    "    return os.path.exists(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoUeopjcRkpx"
   },
   "source": [
    "In this section we build the singleton `NLPDataset`, that contains:\n",
    "1. The original Hugging Face dataset\n",
    "2. The Wikidata entities\n",
    "3. The Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnMKuBqNkBYn"
   },
   "outputs": [],
   "source": [
    "def wikipedia_pages(sitelinks):\n",
    "    result = []\n",
    "    for site_key in sitelinks.keys():\n",
    "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
    "            lang = site_key.replace(\"wiki\", \"\")\n",
    "            result.append(lang)\n",
    "    return result\n",
    "\n",
    "def build_claims(claims):\n",
    "    result = {}\n",
    "    for prop_id, values in claims.items():\n",
    "        result[prop_id] = len(values)\n",
    "    return result\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
    "        self.entity_id = entity_id\n",
    "        self.label = dataset_item['label']\n",
    "        self.name = dataset_item['name']\n",
    "        self.description = dataset_item['description']\n",
    "        self.type = dataset_item['type']\n",
    "        self.category = dataset_item['category']\n",
    "        self.subcategory = dataset_item['subcategory']\n",
    "        self.wiki_text = wiki_text\n",
    "        # Languages\n",
    "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
    "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
    "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
    "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
    "        # Properties\n",
    "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
    "\n",
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "def get_wiki_text(en_wiki):\n",
    "    if not en_wiki:\n",
    "        return None\n",
    "    title = en_wiki[\"title\"]\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
    "    res = requests.get(API_URL, params=params, headers=headers)\n",
    "    json = res.json()\n",
    "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
    "    # Keep the original text as it is.\n",
    "    # The text will be processed in other methods,\n",
    "    # such as processed_dataset#tokenize().\n",
    "    return page.get(\"extract\", \"\")\n",
    "\n",
    "class EntityFactory:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def create(self, item):\n",
    "        entity_id = extract_entity_id(item['item'])\n",
    "        try:\n",
    "            wikidata = self.client.get(entity_id, load=True)\n",
    "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
    "            en_wiki = sitelinks.get(\"enwiki\")\n",
    "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
    "        except Exception as e:\n",
    "            print(\"Error loading id:\", entity_id, e)\n",
    "            return None\n",
    "\n",
    "TRAINING_FILE_NAME = \"training.bin\"\n",
    "VALIDATION_FILE_NAME = \"validation.bin\"\n",
    "\n",
    "def create_set(dataset, factory, limit, file_name):\n",
    "    # apply the limits\n",
    "    if limit is None:\n",
    "        limit = len(dataset)\n",
    "    result = []\n",
    "    for index, item in enumerate(islice(dataset, limit)):\n",
    "        created = factory.create(item)\n",
    "        if created is not None:\n",
    "            result.append(created)\n",
    "        if (index + 1) % 10 == 0:\n",
    "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
    "    return result\n",
    "\n",
    "class NLPDataset:\n",
    "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
    "        if not (file_exists(TRAINING_FILE_NAME)) or not (file_exists(VALIDATION_FILE_NAME)) or force_reload:\n",
    "            # load the project dataset\n",
    "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HF_TOKEN'))\n",
    "            # a factory object is used to create our entities\n",
    "            factory = EntityFactory(Client())\n",
    "\n",
    "            self.training_set = create_set(dataset['train'], factory, training_limit, TRAINING_FILE_NAME)\n",
    "            self.validation_set = create_set(dataset['validation'], factory, validation_limit, VALIDATION_FILE_NAME)\n",
    "            dump(TRAINING_FILE_NAME, self.training_set)\n",
    "            dump(VALIDATION_FILE_NAME, self.validation_set)\n",
    "        else:\n",
    "            # by default load the dataset from a local dump\n",
    "            self.training_set = load(TRAINING_FILE_NAME)\n",
    "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMIbJbNklFtG",
    "outputId": "7db21050-f7af-4131-b7c0-c3a3cf933dc3"
   },
   "outputs": [],
   "source": [
    "nlp_dataset = NLPDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cmm1ekIxRrlM"
   },
   "source": [
    "If the dump files `training.bin` and `validation.bin` are present, the instance is build from the dump.\n",
    "And this is all we need to use the transformer. That is, usually with transformers we don't need to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B_Ck1I2MI5r",
    "outputId": "04de3beb-8d39-4a76-dbb5-f21a9cfdc722"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUREAurO-f48"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, TrainerCallback\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCV8K37DR3mV"
   },
   "source": [
    "In this case we will keep the description and the Wikipedia page text as they are, leaving the burden of taking text processing decisions to the tokenizer. The dataset here is enriched with the Wikipedia text and the labels are mapped to numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OETfZRXAlIo"
   },
   "outputs": [],
   "source": [
    "def build_entity_dict():\n",
    "    entity_dict = {}\n",
    "    for entity in nlp_dataset.training_set:\n",
    "        entity_dict[entity.entity_id] = entity\n",
    "    for entity in nlp_dataset.validation_set:\n",
    "        entity_dict[entity.entity_id] = entity\n",
    "    return entity_dict\n",
    "\n",
    "def label_to_number(label):\n",
    "    if label == 'cultural agnostic':\n",
    "        return 0\n",
    "    if label == 'cultural representative':\n",
    "        return 1\n",
    "    if label == 'cultural exclusive':\n",
    "        return 2\n",
    "    raise ValueError('label not suppoerted: ' + label)\n",
    "\n",
    "class WikiDataset:\n",
    "    def __init__(self):\n",
    "        entity_dict = build_entity_dict()\n",
    "        dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HF_TOKEN'))\n",
    "        # enriching the entities with the wiki pages\n",
    "        def map_labels(sample):\n",
    "            label = sample[\"label\"]\n",
    "            sample[\"label\"] = label_to_number(label)\n",
    "            wiki_id = extract_entity_id(sample[\"item\"])\n",
    "            if wiki_id is not None and wiki_id in entity_dict:\n",
    "                wiki_text = entity_dict[wiki_id].wiki_text\n",
    "                sample[\"wiki_text\"] = wiki_text if type(wiki_text) == str else \"\"\n",
    "            else:\n",
    "                sample[\"wiki_text\"] = \"\"\n",
    "            return sample\n",
    "        self.dataset = dataset.map(map_labels)\n",
    "\n",
    "    def tokenize(self, tokenizer):\n",
    "        def tokenize_function(items):\n",
    "            return tokenizer(items[\"description\"], items[\"wiki_text\"], padding=True, truncation=True)\n",
    "        return self.dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOU2V-U4R7OC"
   },
   "source": [
    "Hugging Faces supports a lot of different encoders, we tried some of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSSAqz3XODXM"
   },
   "outputs": [],
   "source": [
    "LMs = {\n",
    "    \"bigbird\": {\n",
    "        \"model_name\": \"google/bigbird-roberta-base\",\n",
    "        \"max_length\": 4096,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 4\n",
    "    },\n",
    "    \"distilbert\": {\n",
    "        \"model_name\": \"distilbert-base-uncased\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"roberta_base\": {\n",
    "        \"model_name\": \"roberta-base\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 7, # manual early stop\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"roberta_large\": {\n",
    "        \"model_name\": \"roberta-large\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 8,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"xlm_base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 8,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"xlm_large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 8,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"mdeberta_base\": {\n",
    "        \"model_name\": \"microsoft/mdeberta-v3-base\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 8,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \"mdeberta_large\": {\n",
    "        \"model_name\": \"microsoft/mdeberta-v3-large\",\n",
    "        \"max_length\": 512,\n",
    "        \"epochs\": 8,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "}\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "class NLPHyperParams:\n",
    "    def __init__(self):\n",
    "        set_seed(42)\n",
    "        key = \"roberta_base\"\n",
    "        self.language_model_name = LMs[key][\"model_name\"]\n",
    "        self.max_length = LMs[key][\"max_length\"]\n",
    "        self.batch_size = LMs[key][\"batch_size\"]\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.05\n",
    "        self.epochs = LMs[key][\"epochs\"]\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"device: \", self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_UVSPjDSByY"
   },
   "source": [
    "The Hugging Faces APIs are very high level hiding a lot of detail (and complexity) from the final user. Here we load the pretrained encorder (the encoder part of the transformer) and we apply the output embeddings to a feed forward network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48_XcVWgO6qw"
   },
   "outputs": [],
   "source": [
    "class NLPEncoderModel:\n",
    "    def __init__(self, params: NLPHyperParams):\n",
    "        self.params = params\n",
    "        ## Initialize the model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(params.language_model_name, ignore_mismatched_sizes=True,\n",
    "            output_attentions=False, output_hidden_states=False, num_labels=3) # number of the classes\n",
    "        # Load the pretrained tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(params.language_model_name)\n",
    "        # Set the data collator\n",
    "        # Function used to prepare the data before the training.\n",
    "        # the data collator function used here apply a zero-padding on the elements in the batch\n",
    "        # the padding is needed to have a \"full\" form of the batches\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "    def push(self, repo):\n",
    "        self.model.push_to_hub(repo, token=userdata.get('HF_TOKEN_PRO'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sht-zFzbSIZU"
   },
   "source": [
    "We introduces a custom callback to evaluate loss and accuracy of both the training and the validation sets. The idea here is to produce a result similar to the no-transformer part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MCZCK_jPsun"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self, trainer) -> None:\n",
    "        self._trainer = trainer\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.valid_loss = []\n",
    "        self.valid_accuracy = []\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            train_evaluation = self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            self.train_loss.append(train_evaluation['train_loss'])\n",
    "            self.train_accuracy.append(train_evaluation['train_accuracy'])\n",
    "            valid_evaluation = self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset, metric_key_prefix=\"valid\")\n",
    "            self.valid_loss.append(valid_evaluation['valid_loss'])\n",
    "            self.valid_accuracy.append(valid_evaluation['valid_accuracy'])\n",
    "            return control_copy\n",
    "\n",
    "class NLPTrainer:\n",
    "    def __init__(self, params: NLPHyperParams, model: NLPEncoderModel, train_dataset, eval_dataset):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"training_dir\",                      # output directory [Mandatory]\n",
    "            num_train_epochs=params.epochs,                 # total number of training epochs\n",
    "            per_device_train_batch_size=params.batch_size,  # batch size per device during training\n",
    "            warmup_steps=500,                               # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=params.weight_decay,               # strength of weight decay\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            learning_rate=params.learning_rate,             # learning rate\n",
    "            report_to=\"none\",\n",
    "            logging_dir=\"cultural_analysis_logs\"           # use it later to get the training curves\n",
    "        )\n",
    "        self.trainer = Trainer(\n",
    "            model=model.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=model.tokenizer,\n",
    "            data_collator=model.data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        self.callback = CustomCallback(self.trainer)\n",
    "        self.trainer.add_callback(self.callback)\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        self.trainer.train()\n",
    "        return {\n",
    "            \"train_loss\": self.callback.train_loss,\n",
    "            \"train_accuracy\": self.callback.train_accuracy,\n",
    "            \"valid_loss\": self.callback.valid_loss,\n",
    "            \"valid_accuracy\": self.callback.valid_accuracy\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4niH79tlUxY0"
   },
   "source": [
    "Putting all together we train (evaluate) and optionally publish the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IKPI1dUOPnOC",
    "outputId": "0efde1df-eca5-46f6-be35-42e7318779c0"
   },
   "outputs": [],
   "source": [
    "params = NLPHyperParams()\n",
    "dataset = WikiDataset()\n",
    "model = NLPEncoderModel(params)\n",
    "\n",
    "print(\"Tokenize the dataset ...\")\n",
    "tokenized_datasets = dataset.tokenize(model.tokenizer)\n",
    "print(tokenized_datasets)\n",
    "\n",
    "train_ = tokenized_datasets[\"train\"]\n",
    "validation_ = tokenized_datasets[\"validation\"]\n",
    "trainer = NLPTrainer(params, model, train_, validation_)\n",
    "history = trainer.train_and_evaluate()\n",
    "print(history)\n",
    "\n",
    "plt.title(\"MSE Loss - Plot\")\n",
    "plt.plot(history[\"train_loss\"], label=\"training loss\")\n",
    "plt.plot(history[\"valid_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Accuracy - Plot\")\n",
    "plt.plot(history[\"train_accuracy\"], label=\"training accuracy\")\n",
    "plt.plot(history[\"valid_accuracy\"], label=\"validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "13ac63fedc1f4d0cb85b57729c14b209",
      "aeede37d0761437dac5c1bd7f8a6a169",
      "15a8c29b0d3f438a9d05e0990abbf901",
      "5ae2172937fb4d7c8c983690dd36ae97",
      "30ae8526ccbf45e6ab0fe12b7d1ab264",
      "1dfc37ac2cd54b09b23eac38d484160a",
      "955e34ba3af64a478b2f5c843d80ef31",
      "46bd902d0e4f456cb42a6ad3d6a07fc0",
      "2ad8bfe094f6434eb39ac169d6254783",
      "37d79fa604ae410aaad924ce6152dbc6",
      "b52fe1db55934c73840929c68073cec2"
     ]
    },
    "id": "pJWh_KC73JHe",
    "outputId": "4271c085-453a-4880-e02d-bae10f09c48b"
   },
   "outputs": [],
   "source": [
    "# uncomment to push\n",
    "model.push(\"fax4ever/culturalitems-roberta-base\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13ac63fedc1f4d0cb85b57729c14b209": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aeede37d0761437dac5c1bd7f8a6a169",
       "IPY_MODEL_15a8c29b0d3f438a9d05e0990abbf901",
       "IPY_MODEL_5ae2172937fb4d7c8c983690dd36ae97"
      ],
      "layout": "IPY_MODEL_30ae8526ccbf45e6ab0fe12b7d1ab264",
      "tabbable": null,
      "tooltip": null
     }
    },
    "15a8c29b0d3f438a9d05e0990abbf901": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_46bd902d0e4f456cb42a6ad3d6a07fc0",
      "max": 498615900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ad8bfe094f6434eb39ac169d6254783",
      "tabbable": null,
      "tooltip": null,
      "value": 498615900
     }
    },
    "1dfc37ac2cd54b09b23eac38d484160a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ad8bfe094f6434eb39ac169d6254783": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30ae8526ccbf45e6ab0fe12b7d1ab264": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37d79fa604ae410aaad924ce6152dbc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46bd902d0e4f456cb42a6ad3d6a07fc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ae2172937fb4d7c8c983690dd36ae97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_37d79fa604ae410aaad924ce6152dbc6",
      "placeholder": "​",
      "style": "IPY_MODEL_b52fe1db55934c73840929c68073cec2",
      "tabbable": null,
      "tooltip": null,
      "value": " 499M/499M [00:28&lt;00:00, 19.4MB/s]"
     }
    },
    "955e34ba3af64a478b2f5c843d80ef31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "aeede37d0761437dac5c1bd7f8a6a169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1dfc37ac2cd54b09b23eac38d484160a",
      "placeholder": "​",
      "style": "IPY_MODEL_955e34ba3af64a478b2f5c843d80ef31",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors: 100%"
     }
    },
    "b52fe1db55934c73840929c68073cec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef88771",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation\n",
    "\n",
    "In this notebook we will evaluate the models we produced for sentence splitting,\n",
    "on the same domain (Quarantana - Manzoni) and out of domain (on novels from different authors of the same time).\n",
    "We copied and slightly adapted the golden splitting novels from [Sentence Splitting Manzoni annotated dataset](https://github.com/RacheleSprugnoli/Sentence_Splitting_Manzoni/tree/main/test-novels).\n",
    "\n",
    "\n",
    "Install the required libraries in your virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe62fc",
   "metadata": {},
   "source": [
    "Import all libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8e03e",
   "metadata": {},
   "source": [
    "Before proceeding, make the run as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed() # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e62fcc",
   "metadata": {},
   "source": [
    "List the fine-tuned (based on embedding models) models we want to test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"bert-base-cased-sentence-splitter\",\n",
    "    \"ModernBERT-base-ita-sentence-splitter\",\n",
    "    \"bert-base-italian-xxl-cased-sentence-splitter\",\n",
    "    \"xlm-roberta-base-sentence-splitter\",\n",
    "    \"xlm-roberta-large-sentence-splitter\",\n",
    "    \"electra-base-italian-xxl-cased-discriminator-sentence-splitter\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1c14c",
   "metadata": {},
   "source": [
    "We want to evaluate the f1 score of the splitting prediction compared.\n",
    "\n",
    "First of all, from a prediction and from a golden-annotated novel fragment we want to produce label.\n",
    "The labels produced for the evaluation here are different from the ones used to fine-tune the models:\n",
    "for simplicity of the implementation we have a label for each character instead of a label for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2142efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_eval(inference_pipeline, sequence_lines):\n",
    "    sequence = \" \".join(sequence_lines)\n",
    "    prediction = inference_pipeline(sequence)\n",
    "    prediction_labels = labels_from_prediction(prediction)\n",
    "    golden_labels = labels_from_novel(sequence_lines)\n",
    "\n",
    "    if (len(prediction_labels) < len(golden_labels)):\n",
    "        print(\"Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\")\n",
    "        golden_labels = golden_labels[:len(prediction_labels)]\n",
    "\n",
    "    return prediction_labels, golden_labels\n",
    "\n",
    "def labels_from_prediction(prediction):\n",
    "    ones = {}\n",
    "    for label in prediction:\n",
    "        if label[\"entity_group\"] == \"LABEL_1\":\n",
    "            for i in range(label[\"start\"], label[\"end\"]):\n",
    "                ones[i] = 1\n",
    "\n",
    "    first = prediction[0]\n",
    "    last = prediction[-1]\n",
    "    labels = []\n",
    "    for i in range(first[\"start\"], last[\"end\"]):\n",
    "        if i in ones:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels_from_novel(novel_lines):\n",
    "    labels = []\n",
    "    for i, novel_line in enumerate(novel_lines):\n",
    "        for _ in range(len(novel_line) - 1):\n",
    "            labels.append(0)\n",
    "        labels.append(1)\n",
    "        if i < len(novel_lines) - 1:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694a560",
   "metadata": {},
   "source": [
    "The golden-annotated novel is grouped in chunk of equal size of lines.\n",
    "\n",
    "From each chunk we evaluate the f1, comparing the labels generated from the golden annotated chunk,\n",
    "with the prediction made by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c37ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model_name: str, novel_path: Path, num_lines_for_eval: int):\n",
    "    model_checkpoint = \"fax4ever/\" + model_name\n",
    "    inference_pipeline = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
    "    novel_lines = novel_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    return eval(inference_pipeline, novel_lines, num_lines_for_eval)\n",
    "\n",
    "def eval(inference_pipeline, novel_lines, num_lines_for_eval):\n",
    "    metric = evaluate.load(\"f1\", average=\"binary\")\n",
    "\n",
    "    grouped_lines = np.array_split(novel_lines, groups(novel_lines, num_lines_for_eval))\n",
    "    for sequence_lines in grouped_lines:\n",
    "        prediction_labels, golden_labels = sequence_eval(inference_pipeline, sequence_lines)\n",
    "        metric.add_batch(predictions=prediction_labels, references=golden_labels)\n",
    "\n",
    "    return metric.compute()  \n",
    "\n",
    "def groups(novel_lines, num_lines_for_eval):\n",
    "    groups = len(novel_lines) / num_lines_for_eval\n",
    "    if len(novel_lines) % num_lines_for_eval != 0:\n",
    "        groups += 1\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950f2c6",
   "metadata": {},
   "source": [
    "We test in-domain novel (Quarantana) and 3 novels from different authors of the same period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    novel = \"Cuore-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 6)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Malavoglia-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 3)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Pinocchio-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 7)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Quarantana-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 5)\n",
    "    print(model_name, novel, value[\"f1\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

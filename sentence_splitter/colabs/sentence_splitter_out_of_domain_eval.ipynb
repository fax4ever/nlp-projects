{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef88771",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation\n",
    "\n",
    "In this notebook we will evaluate the models we produced for sentence splitting,\n",
    "on the same domain (Quarantana - Manzoni) and out of domain (on novels from different authors of the same time).\n",
    "We copied and slightly adapted the golden splitting novels from [Sentence Splitting Manzoni annotated dataset](https://github.com/RacheleSprugnoli/Sentence_Splitting_Manzoni/tree/main/test-novels).\n",
    "\n",
    "\n",
    "Install the required libraries in your virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c58f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/fax/.local/lib/python3.13/site-packages (25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/fax/.local/lib/python3.13/site-packages (4.53.3)\n",
      "Requirement already satisfied: evaluate in /home/fax/.local/lib/python3.13/site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.13/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/fax/.local/lib/python3.13/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib64/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib64/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/fax/.local/lib/python3.13/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/fax/.local/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/fax/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/fax/.local/lib/python3.13/site-packages (from evaluate) (3.5.1)\n",
      "Requirement already satisfied: dill in /home/fax/.local/lib/python3.13/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/lib64/python3.13/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/fax/.local/lib/python3.13/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/fax/.local/lib/python3.13/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/lib64/python3.13/site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/lib64/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.10.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/lib64/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/lib64/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/lib64/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.13.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3.13/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.13/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe62fc",
   "metadata": {},
   "source": [
    "Import all libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fb30d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fax/code/nlp-projects/sentence_splitter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8e03e",
   "metadata": {},
   "source": [
    "Before proceeding, make the run as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397cfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed() # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e62fcc",
   "metadata": {},
   "source": [
    "List the fine-tuned (based on embedding models) models we want to test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9739f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"bert-base-cased-sentence-splitter\",\n",
    "    \"ModernBERT-base-ita-sentence-splitter\",\n",
    "    \"bert-base-italian-xxl-cased-sentence-splitter\",\n",
    "    \"xlm-roberta-base-sentence-splitter\",\n",
    "    \"xlm-roberta-large-sentence-splitter\",\n",
    "    \"electra-base-italian-xxl-cased-discriminator-sentence-splitter\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1c14c",
   "metadata": {},
   "source": [
    "We want to evaluate the f1 score of the splitting prediction compared.\n",
    "\n",
    "First of all, from a prediction and from a golden-annotated novel fragment we want to produce label.\n",
    "The labels produced for the evaluation here are different from the ones used to fine-tune the models:\n",
    "for simplicity of the implementation we have a label for each character instead of a label for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2142efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_eval(inference_pipeline, sequence_lines):\n",
    "    sequence = \" \".join(sequence_lines)\n",
    "    prediction = inference_pipeline(sequence)\n",
    "    prediction_labels = labels_from_prediction(prediction)\n",
    "    golden_labels = labels_from_novel(sequence_lines)\n",
    "\n",
    "    if (len(prediction_labels) < len(golden_labels)):\n",
    "        print(\"Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\")\n",
    "        golden_labels = golden_labels[:len(prediction_labels)]\n",
    "\n",
    "    return prediction_labels, golden_labels\n",
    "\n",
    "def labels_from_prediction(prediction):\n",
    "    ones = {}\n",
    "    for label in prediction:\n",
    "        if label[\"entity_group\"] == \"LABEL_1\":\n",
    "            for i in range(label[\"start\"], label[\"end\"]):\n",
    "                ones[i] = 1\n",
    "\n",
    "    first = prediction[0]\n",
    "    last = prediction[-1]\n",
    "    labels = []\n",
    "    for i in range(first[\"start\"], last[\"end\"]):\n",
    "        if i in ones:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels_from_novel(novel_lines):\n",
    "    labels = []\n",
    "    for i, novel_line in enumerate(novel_lines):\n",
    "        for _ in range(len(novel_line) - 1):\n",
    "            labels.append(0)\n",
    "        labels.append(1)\n",
    "        if i < len(novel_lines) - 1:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694a560",
   "metadata": {},
   "source": [
    "The golden-annotated novel is grouped in chunk of equal size of lines.\n",
    "\n",
    "From each chunk we evaluate the f1, comparing the labels generated from the golden annotated chunk,\n",
    "with the prediction made by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c37ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model_name: str, novel_path: Path, num_lines_for_eval: int):\n",
    "    model_checkpoint = \"fax4ever/\" + model_name\n",
    "    inference_pipeline = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
    "    novel_lines = novel_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    return eval(inference_pipeline, novel_lines, num_lines_for_eval)\n",
    "\n",
    "def eval(inference_pipeline, novel_lines, num_lines_for_eval):\n",
    "    metric = evaluate.load(\"f1\", average=\"binary\")\n",
    "\n",
    "    grouped_lines = np.array_split(novel_lines, groups(novel_lines, num_lines_for_eval))\n",
    "    for sequence_lines in grouped_lines:\n",
    "        prediction_labels, golden_labels = sequence_eval(inference_pipeline, sequence_lines)\n",
    "        metric.add_batch(predictions=prediction_labels, references=golden_labels)\n",
    "\n",
    "    return metric.compute()  \n",
    "\n",
    "def groups(novel_lines, num_lines_for_eval):\n",
    "    groups = len(novel_lines) / num_lines_for_eval\n",
    "    if len(novel_lines) % num_lines_for_eval != 0:\n",
    "        groups += 1\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950f2c6",
   "metadata": {},
   "source": [
    "We test in-domain novel (Quarantana) and 3 novels from different authors of the same period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e8b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased-sentence-splitter Cuore-GOLD.txt 0.9364161849710982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased-sentence-splitter Malavoglia-GOLD.txt 0.782608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased-sentence-splitter Pinocchio-GOLD.txt 0.8854166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\n",
      "Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\n",
      "bert-base-cased-sentence-splitter Quarantana-GOLD.txt 0.9859154929577465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Cuore-GOLD.txt 0.8144329896907216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Malavoglia-GOLD.txt 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Pinocchio-GOLD.txt 0.5603112840466926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Quarantana-GOLD.txt 0.9130434782608695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Cuore-GOLD.txt 0.9485714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Malavoglia-GOLD.txt 0.9120879120879121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Pinocchio-GOLD.txt 0.8773584905660378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Quarantana-GOLD.txt 0.9938271604938271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Cuore-GOLD.txt 0.24242424242424243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Malavoglia-GOLD.txt 0.21153846153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Pinocchio-GOLD.txt 0.5426356589147286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Quarantana-GOLD.txt 0.3952380952380952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Cuore-GOLD.txt 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Malavoglia-GOLD.txt 0.3559322033898305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Pinocchio-GOLD.txt 0.6618705035971223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Quarantana-GOLD.txt 0.5274725274725275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Cuore-GOLD.txt 0.9385474860335196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Malavoglia-GOLD.txt 0.9347826086956522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Pinocchio-GOLD.txt 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Quarantana-GOLD.txt 0.9938461538461538\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    novel = \"Cuore-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 6)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Malavoglia-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 3)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Pinocchio-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 7)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Quarantana-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 5)\n",
    "    print(model_name, novel, value[\"f1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803724cc",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation\n",
    "\n",
    "## Test set / Generative models Notebook\n",
    "\n",
    "In this notebook we're going to use a fine tuned sentence splitting LLM model (based Minerva 7B instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e3a31",
   "metadata": {},
   "source": [
    "Install the liberaries on the local virtual environment.\n",
    "We asked for specific versions to enforce maximum reproducibiliy for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers==4.56.1 evaluate==0.4.5 torch==2.7.0 unsloth==2025.9.1 ipywidgets==8.1.7 numpy==2.3.2 pandas==2.3.2 datasets==3.6.0 jupyter==1.1.1 scikit-learn==1.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda69806",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb35c8e",
   "metadata": {},
   "source": [
    "Before proceeding, make the run as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baeeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 777\n",
    "\n",
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_STATE) # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da81171",
   "metadata": {},
   "source": [
    "Reuse the `Prompt` class from the training notebook.\n",
    "Here only the `question` method will be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, input_text):\n",
    "        self.input_text = input_text\n",
    "\n",
    "    def instruction(self):\n",
    "        return f\"\"\"Dividi il seguente testo italiano in frasi. Per favore rispondi con una frase per riga. Grazie.\n",
    "\n",
    "Testo: {self.input_text}\n",
    "\"\"\"\n",
    "\n",
    "    def conversation(self, output_text):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "            {\"role\" : \"assistant\", \"content\" : output_text},\n",
    "        ]\n",
    "\n",
    "    def question(self):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        'fax4ever/' + model_name, \n",
    "        load_in_4bit=True, \n",
    "        dtype=None, \n",
    "        max_seq_length=512\n",
    "    )\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer    \n",
    "\n",
    "def use_model(model, tokenizer, input_text):\n",
    "    question = tokenizer.apply_chat_template(\n",
    "        [Prompt(input_text).question()], \n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        enable_thinking = False, # Disable thinking\n",
    "    )\n",
    "\n",
    "    return model.generate(\n",
    "        **tokenizer(question, return_tensors = \"pt\").to(\"cuda\"),\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7eb3e",
   "metadata": {},
   "source": [
    "The LLM-based models we fine tuned are:\n",
    "\n",
    "1. Minerva-7B-instruct-v1.0-sentence-splitter\n",
    "2. qwen3-4b-unsloth-bnb-4bit-sentence-splitter\n",
    "3. mistral-7b-instruct-v0.3-bnb-4bit-sentence-splitter\n",
    "4. meta-llama-3.1-8b-instruct-unsloth-bnb-4bit-sentence-splitter\n",
    "\n",
    "We choose to produce label for the (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\"Minerva-7B-instruct-v1.0-sentence-splitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with fax4ever/sentence-splitter-ood-192 we produce more than 512 tokens!\n",
    "dataset_dict = load_dataset(\"fax4ever/sentence-splitter-ood-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_sequence(words):\n",
    "    input_text = \" \".join(words)\n",
    "    input_text = input_text.replace(\" ,\", \",\")\n",
    "    input_text = input_text.replace(\" .\", \".\")\n",
    "    input_text = input_text.replace(\" ?\", \"?\")\n",
    "    input_text = input_text.replace(\" !\", \"!\")\n",
    "    input_text = input_text.replace(\" :\", \":\")\n",
    "    input_text = input_text.replace(\" ;\", \";\")\n",
    "    input_text = input_text.replace(\"' \", \"'\")\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23635c80",
   "metadata": {},
   "source": [
    "Produce sentence splitting labels from a LLM is not a super easy task according to our analysis.\n",
    "So we created an utility class to produce labels for the test set words using the LLM output.\n",
    "\n",
    "To have a perfect align we need to manually change the output of the LLM with very few replacements:\n",
    "\n",
    "1. ... => …\n",
    "2. mise => messe\n",
    "3. trasfigurato => trasfigurito\n",
    "\n",
    "The tre dots can be writen as a `…` single caracter in place of the three dots caracters `...`.\n",
    "\n",
    "Messe can be used in placed of mise, see https://www.scholingua.com/it/it/coniugazione/mettersi.\n",
    "Probabily the former is an older form.\n",
    "\n",
    "Transfigurito looks an alternative way of saying transfigurato, see https://www.treccani.it/vocabolario/ricerca/trasfigurito/.\n",
    "\n",
    "We can see those as small allucinations and we want to tollerate.\n",
    "Other allucincations will produce an `AllucinationException` that will be logged to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllucinationException(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinervaLabels:\n",
    "  def __init__(self, minerva_output:str, words:list):\n",
    "    self.words = words\n",
    "    self.words_joined = \"\".join(words)\n",
    "    self.minerva_output = minerva_output\n",
    "    self.sentences = self._create_sentences()\n",
    "    self.aligned_sentences = self._aligned_sentences()\n",
    "    self._check_alinged_sentences() # Sanity check\n",
    "\n",
    "  def _create_sentences(self):\n",
    "    import re\n",
    "    sentences = []\n",
    "    for line in self.minerva_output.split(\"\\n\"):\n",
    "      # Look for lines that start with number followed by dot and space\n",
    "      match = re.match(r'^(\\d+)\\.\\s+(.*)', line)\n",
    "      if match:\n",
    "        sentence = match.group(2)  # Extract the sentence part after \"NUMBER. \"\n",
    "        # Clean up Minerva control tokens like <|eot_id|>\n",
    "        sentence = re.sub(r'<\\|[^|]+\\|>', '', sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "  def _aligned_sentences(self):\n",
    "    return [self._align_sentence(sentence) for sentence in self.sentences]\n",
    "\n",
    "  def _align_sentence(self, sentence):\n",
    "    sentence = sentence.replace(\" \", \"\")\n",
    "    sentence = sentence.replace(\"...\", \"…\")\n",
    "    sentence = sentence.replace(\"mise\", \"messe\")\n",
    "    sentence = sentence.replace(\"trasfigurato\", \"trasfigurito\")\n",
    "    if sentence in self.words_joined:\n",
    "      return sentence\n",
    "    else:\n",
    "      # in this case the model is hallucinating altering a sentence or producing a sentence that does not exist\n",
    "      raise AllucinationException(f\"Sentence {sentence} not found in {self.words_joined}\")\n",
    "\n",
    "  def _check_alinged_sentences(self):\n",
    "    aligned_join = \"\".join(self.aligned_sentences)\n",
    "    if not aligned_join == self.words_joined:\n",
    "      # in this case the model is hallucinating not producing a sentence passed in input\n",
    "      raise AllucinationException(f\"Aligned sentences {self.aligned_sentences} do not match words {self.words_joined}\")\n",
    "\n",
    "  def aligned_labels(self, golden_labels):\n",
    "    labels = [0] * len(self.words)\n",
    "\n",
    "    index = 0\n",
    "    split_indexes = set()\n",
    "    for sentence in self.aligned_sentences:\n",
    "      length = len(sentence)\n",
    "      index += length\n",
    "      split_indexes.add(index)\n",
    "    \n",
    "    index = 0\n",
    "    for i, word in enumerate(self.words):\n",
    "      length = len(word)\n",
    "      index += length\n",
    "      if index in split_indexes:\n",
    "        labels[i] = 1\n",
    "\n",
    "    # The last word can be a 1 or a 0,\n",
    "    # the model is not asked to predict the last word, so we use the golden labels\n",
    "    labels[-1] = golden_labels[-1]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "all_golden_labels = []\n",
    "all_minerva_labels = []\n",
    "minerva_f1 = evaluate.load(\"f1\", average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataset_dict[\"test\"].iter(batch_size=1)):\n",
    "    words = batch[\"tokens\"][0]\n",
    "    golden_labels = batch[\"labels\"][0]\n",
    "\n",
    "    output = use_model(model, tokenizer, words_to_sequence(words)).cpu()\n",
    "    minerva_output = tokenizer.decode(output[0])\n",
    "\n",
    "    try:\n",
    "        minerva_labels_helper = MinervaLabels(minerva_output, words)\n",
    "    except AllucinationException as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    minerva_labels = minerva_labels_helper.aligned_labels(golden_labels)\n",
    "\n",
    "    print(f\"Batch {i}\")\n",
    "    print(words)\n",
    "    print(golden_labels)\n",
    "    print(minerva_labels)\n",
    "\n",
    "    all_words.extend(words)\n",
    "    all_golden_labels.extend(golden_labels)\n",
    "    all_minerva_labels.extend(minerva_labels)\n",
    "    minerva_f1.add_batch(predictions=minerva_labels, references=golden_labels)\n",
    "\n",
    "print(\"Minerva-7B-instruct-v1.0-sentence-splitter F1: \", minerva_f1.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ef4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a pandas dataframe with the results\n",
    "mb_results = pd.DataFrame({\n",
    "    \"token\": all_words,\n",
    "    \"label\": all_minerva_labels,\n",
    "})\n",
    "\n",
    "# save the results\n",
    "mb_results.to_csv(\"Lost_in_language_recognition-hw2_split-Minerva-7B-based.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803724cc",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation (Generative models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers evaluate torch numpy pandas datasets jupyter unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda69806",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb35c8e",
   "metadata": {},
   "source": [
    "Before proceeding, make the run as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baeeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 777\n",
    "\n",
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_STATE) # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da81171",
   "metadata": {},
   "source": [
    "Reuse the `Prompt` class from the training notebook.\n",
    "Here only the `question` method will be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, input_text):\n",
    "        self.input_text = input_text\n",
    "\n",
    "    def instruction(self):\n",
    "        return f\"\"\"Dividi il seguente testo italiano in frasi. Per favore rispondi con una frase per riga. Grazie.\n",
    "\n",
    "Testo: {self.input_text}\n",
    "\"\"\"\n",
    "\n",
    "    def conversation(self, output_text):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "            {\"role\" : \"assistant\", \"content\" : output_text},\n",
    "        ]\n",
    "\n",
    "    def question(self):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c468e",
   "metadata": {},
   "source": [
    "Let's test (this time manually) how the trained LLM-based models perform on a given text\n",
    "using an out of domain novel (Cuore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Non era un legno di lusso, ma un semplice pezzo\n",
    "da catasta, di quelli che d’inverno si mettono nelle\n",
    "stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze.\n",
    "Non so come andasse, ma il fatto gli è che un bel\n",
    "giorno questo pezzo di legno capitò nella bottega\n",
    "di un vecchio falegname, il quale aveva nome mastr’Antonio, se non che tutti lo chiamavano maestro\n",
    "Ciliegia, per via della punta del suo naso, che era\n",
    "sempre lustra e paonazza, come una ciliegia matura.\n",
    "Appena maestro Ciliegia ebbe visto quel pezzo di\n",
    "legno, si rallegrò tutto; e dandosi una fregatina di\n",
    "mani per la contentezza, borbottò a mezza voce:\n",
    "\"Questo legno è capitato a tempo; voglio servirmene per fare una gamba di tavolino.\" \n",
    "\"\"\"\n",
    "input_text = input_text.splitlines()\n",
    "input_text = \" \".join(input_text)\n",
    "input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84ec7d",
   "metadata": {},
   "source": [
    "Each model is loaded from Hugging Face and used to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        'fax4ever/' + name, \n",
    "        load_in_4bit=True, \n",
    "        dtype=None, \n",
    "        max_seq_length=512\n",
    "    )\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    question = tokenizer.apply_chat_template(\n",
    "        [Prompt(input_text).question()], \n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        enable_thinking = False, # Disable thinking\n",
    "    )\n",
    "\n",
    "    model.generate(\n",
    "        **tokenizer(question, return_tensors = \"pt\").to(\"cuda\"),\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66fd04",
   "metadata": {},
   "source": [
    "Testing one model at a time (commenting the others) to avoid memory issues,\n",
    "restarting the Kernel to free the GPU memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(\"qwen3-4b-unsloth-bnb-4bit-sentence-splitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(\"meta-llama-3.1-8b-instruct-unsloth-bnb-4bit-sentence-splitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(\"mistral-7b-instruct-v0.3-bnb-4bit-sentence-splitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90024bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(\"Minerva-7B-instruct-v1.0-sentence-splitter\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

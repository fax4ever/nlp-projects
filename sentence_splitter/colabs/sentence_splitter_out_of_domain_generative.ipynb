{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803724cc",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation (Generative models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers==4.56.1 evaluate==0.4.5 torch==2.7.0 unsloth==2025.9.1 ipywidgets==8.1.7 numpy==2.3.2 pandas==2.3.2 datasets==3.6.0 jupyter==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda69806",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb35c8e",
   "metadata": {},
   "source": [
    "Before proceeding, make the run as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baeeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 777\n",
    "\n",
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_STATE) # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da81171",
   "metadata": {},
   "source": [
    "Reuse the `Prompt` class from the training notebook.\n",
    "Here only the `question` method will be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, input_text):\n",
    "        self.input_text = input_text\n",
    "\n",
    "    def instruction(self):\n",
    "        return f\"\"\"Dividi il seguente testo italiano in frasi. Per favore rispondi con una frase per riga. Grazie.\n",
    "\n",
    "Testo: {self.input_text}\n",
    "\"\"\"\n",
    "\n",
    "    def conversation(self, output_text):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "            {\"role\" : \"assistant\", \"content\" : output_text},\n",
    "        ]\n",
    "\n",
    "    def question(self):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        'fax4ever/' + model_name, \n",
    "        load_in_4bit=True, \n",
    "        dtype=None, \n",
    "        max_seq_length=512\n",
    "    )\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer    \n",
    "\n",
    "def use_model(model, tokenizer, input_text):\n",
    "    question = tokenizer.apply_chat_template(\n",
    "        [Prompt(input_text).question()], \n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        enable_thinking = False, # Disable thinking\n",
    "    )\n",
    "\n",
    "    return model.generate(\n",
    "        **tokenizer(question, return_tensors = \"pt\").to(\"cuda\"),\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7eb3e",
   "metadata": {},
   "source": [
    "The LLM-based models we fine tuned are:\n",
    "\n",
    "1. Minerva-7B-instruct-v1.0-sentence-splitter\n",
    "2. qwen3-4b-unsloth-bnb-4bit-sentence-splitter\n",
    "3. mistral-7b-instruct-v0.3-bnb-4bit-sentence-splitter\n",
    "4. meta-llama-3.1-8b-instruct-unsloth-bnb-4bit-sentence-splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\"Minerva-7B-instruct-v1.0-sentence-splitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with fax4ever/sentence-splitter-ood-192 we produce more than 512 tokens!\n",
    "dataset_dict = load_dataset(\"fax4ever/sentence-splitter-ood-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_sequence(words):\n",
    "    input_text = \" \".join(words)\n",
    "    input_text = input_text.replace(\" ,\", \",\")\n",
    "    input_text = input_text.replace(\" .\", \".\")\n",
    "    input_text = input_text.replace(\" ?\", \"?\")\n",
    "    input_text = input_text.replace(\" !\", \"!\")\n",
    "    input_text = input_text.replace(\" :\", \":\")\n",
    "    input_text = input_text.replace(\" ;\", \";\")\n",
    "    input_text = input_text.replace(\"' \", \"'\")\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_dict[\"test\"].iter(batch_size=1):\n",
    "    words = batch[\"tokens\"][0]\n",
    "    golden_labels = batch[\"labels\"][0]\n",
    "\n",
    "    output = use_model(model, tokenizer, words_to_sequence(words)).cpu()\n",
    "    text = tokenizer.decode(output[0])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

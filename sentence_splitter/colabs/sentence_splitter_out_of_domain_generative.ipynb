{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803724cc",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation (Generative models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda69806",
   "metadata": {},
   "source": [
    "Import all libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baeeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 777\n",
    "\n",
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_STATE) # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, input_text):\n",
    "        self.input_text = input_text\n",
    "\n",
    "    def instruction(self):\n",
    "        return f\"\"\"Dividi il seguente testo italiano in frasi. Per favore rispondi con una frase per riga. Grazie.\n",
    "\n",
    "Testo: {self.input_text}\n",
    "\"\"\"\n",
    "\n",
    "    def conversation(self, output_text):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "            {\"role\" : \"assistant\", \"content\" : output_text},\n",
    "        ]\n",
    "\n",
    "    def question(self):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6607978",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained('fax4ever/Minerva-7B-instruct-v1.0-sentence-splitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Non era un legno di lusso, ma un semplice pezzo\n",
    "da catasta, di quelli che d’inverno si mettono nelle\n",
    "stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze.\n",
    "Non so come andasse, ma il fatto gli è che un bel\n",
    "giorno questo pezzo di legno capitò nella bottega\n",
    "di un vecchio falegname, il quale aveva nome mastr’Antonio, se non che tutti lo chiamavano maestro\n",
    "Ciliegia, per via della punta del suo naso, che era\n",
    "sempre lustra e paonazza, come una ciliegia matura.\n",
    "Appena maestro Ciliegia ebbe visto quel pezzo di\n",
    "legno, si rallegrò tutto; e dandosi una fregatina di\n",
    "mani per la contentezza, borbottò a mezza voce:\n",
    "\"Questo legno è capitato a tempo; voglio servirmene per fare una gamba di tavolino.\" \n",
    "\"\"\"\n",
    "input_text = input_text.splitlines()\n",
    "input_text = \" \".join(input_text)\n",
    "\n",
    "print(input_text)\n",
    "\n",
    "question = tokenizer.apply_chat_template(\n",
    "    [Prompt(input_text).question()], \n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer(question, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab06da8",
   "metadata": {},
   "source": [
    "# Sentence Splitter using an Embedding Model\n",
    "\n",
    "Install the required libraries on the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba93cf-d562-40aa-9e77-b826fe0b3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch numpy pandas datasets jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd513f",
   "metadata": {},
   "source": [
    "Let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0e909-635d-4953-b6f0-1db100b79730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6713e6e",
   "metadata": {},
   "source": [
    "First of all, let's verify we support accellerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4a80d-c9a5-4949-b7d0-6dac672c45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d5b93",
   "metadata": {},
   "source": [
    "Before doing everything else try to make this run as much as deternistic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c03d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed() # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22155736",
   "metadata": {},
   "source": [
    "## PART ONE: Create the dataset\n",
    "\n",
    "In this section we're going to create a standard Hugging Face dataset from the `cvs` files: `data/manzoni_dev_tokens.csv` and `data/manzoni_train_tokens.csv`.\n",
    "\n",
    "The output will be the available at [fax4ever/manzoni-192](https://huggingface.co/datasets/fax4ever/manzoni-192).\n",
    "\n",
    "## Our first hyperparameter\n",
    "\n",
    "Basically, the original csv files report a text that is supposed to splitted in portions that can be passed as input to the encoder model. Typically the max number of tokens that can be passed to the encoder model is 512 (for instance this is true for BERT).\n",
    "Now we should think about the fact that for each word of the text in general the tokenizer of the model will produce one or more tokens.\n",
    "A strategy could be split the texts in order to use the maximum number of token possible, this is proven to be not optimal sometimes.\n",
    "So the number of words we want to put on each input will be our first hyberparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa7d2e-00af-41b7-b24f-99065b5b59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 192 # Number of words to put on each input of the encoder model\n",
    "\n",
    "def group_into_sequences(df, seq_len=SIZE):\n",
    "    tokens = df['token'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "    \n",
    "    # Group into sequences of seq_len\n",
    "    token_seqs = [tokens[i:i+seq_len] for i in range(0, len(tokens), seq_len) if len(tokens[i:i+seq_len]) == seq_len]\n",
    "    label_seqs = [labels[i:i+seq_len] for i in range(0, len(labels), seq_len) if len(labels[i:i+seq_len]) == seq_len]\n",
    "    \n",
    "    return {'tokens': token_seqs, 'labels': label_seqs}\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"data/manzoni_train_tokens.csv\")  # token,label\n",
    "validation = pd.read_csv(\"data/manzoni_dev_tokens.csv\")  # token,label\n",
    "\n",
    "# Group into sequences of SIZE\n",
    "train_grouped = group_into_sequences(train)\n",
    "validation_grouped = group_into_sequences(validation)\n",
    "\n",
    "print(f\"Train: {len(train_grouped['tokens'])} sequences of {SIZE} tokens each\")\n",
    "print(f\"Validation: {len(validation_grouped['tokens'])} sequences of {SIZE} tokens each\")\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_grouped)\n",
    "validation_dataset = Dataset.from_dict(validation_grouped)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset  # Using 'validation' as the standard name\n",
    "})\n",
    "dataset_dict.push_to_hub(f\"fax4ever/manzoni-{SIZE}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4cce7",
   "metadata": {},
   "source": [
    "The result is published as a Hugging Face dataset, so standard Hugging Face API could be applied on it.\n",
    "That is the benefit of follow an open standard!\n",
    "\n",
    "## PART TWO: Tokenize the dataset\n",
    "\n",
    "In the tokenization process each word of each input will become one or more tokens.\n",
    "First of all, we need to define some contanstants.\n",
    "In the contants we refect the convetion we implicitly found in the csv files. The 1 denotes the end and the begin of a new sentence, while the 0 will be used to denote all the other tokens. Special tokens denoting start and end of the input-encoding sequences will be labeled with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_OF_SENTENCE = 1\n",
    "NOT_END_OF_SENTENCE = 0\n",
    "LABEL_FOR_START_END_OF_SEQUENCE = NOT_END_OF_SENTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049da04b",
   "metadata": {},
   "source": [
    "The original dataset provides labels for each `word`, in order to have label for each `token` we introduced an utility function:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab24b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab06da8",
   "metadata": {},
   "source": [
    "# Sentence Splitter using an Embedding Model\n",
    "\n",
    "Install the required libraries in the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba93cf-d562-40aa-9e77-b826fe0b3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch numpy pandas datasets jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd513f",
   "metadata": {},
   "source": [
    "Let's import everything we need:\n",
    "\n",
    "(doing it at the beginning to fail fast in case we need something else to install in our virtual environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0e909-635d-4953-b6f0-1db100b79730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, \n",
    "                          TrainingArguments, Trainer, pipeline)\n",
    "from typing import Union, Any, Optional\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6713e6e",
   "metadata": {},
   "source": [
    "First of all, let's verify we support accelerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4a80d-c9a5-4949-b7d0-6dac672c45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d5b93",
   "metadata": {},
   "source": [
    "Before doing everything else try to make this run as much as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c03d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed() # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22155736",
   "metadata": {},
   "source": [
    "## PART ONE: Create the dataset\n",
    "\n",
    "In this section we're going to create a standard Hugging Face dataset from the `CSV` files: `data/manzoni_dev_tokens.csv` and `data/manzoni_train_tokens.csv`.\n",
    "\n",
    "The output will be available at [fax4ever/manzoni-192](https://huggingface.co/datasets/fax4ever/manzoni-192).\n",
    "\n",
    "## Our first hyperparameter\n",
    "\n",
    "Basically, the original csv files report a text that is supposed to be split in portions that can be passed as input to the encoder model. Typically the max number of tokens that can be passed to the encoder model is 512 (for instance this is true for BERT).\n",
    "Now we should think about the fact that for each word of the text in general the tokenizer of the model will produce one or more tokens.\n",
    "A strategy could be split the texts in order to use the maximum number of token possible, this is proven to be not optimal sometimes.\n",
    "So the number of words we want to put on each input will be our first hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 192 # Number of words to put on each input of the encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f2acb",
   "metadata": {},
   "source": [
    "In the following code, we create the dataset.\n",
    "\n",
    "The result is published as a Hugging Face dataset, so standard Hugging Face API could be applied on it.\n",
    "That is the benefit of following an open standard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa7d2e-00af-41b7-b24f-99065b5b59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_into_sequences(df, seq_len=SIZE):\n",
    "    tokens = df['token'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "    \n",
    "    # Group into sequences of seq_len\n",
    "    token_seqs = [tokens[i:i+seq_len] for i in range(0, len(tokens), seq_len) if len(tokens[i:i+seq_len]) == seq_len]\n",
    "    label_seqs = [labels[i:i+seq_len] for i in range(0, len(labels), seq_len) if len(labels[i:i+seq_len]) == seq_len]\n",
    "    \n",
    "    return {'tokens': token_seqs, 'labels': label_seqs}\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"data/manzoni_train_tokens.csv\")  # token,label\n",
    "validation = pd.read_csv(\"data/manzoni_dev_tokens.csv\")  # token,label\n",
    "\n",
    "# Group into sequences of SIZE\n",
    "train_grouped = group_into_sequences(train)\n",
    "validation_grouped = group_into_sequences(validation)\n",
    "\n",
    "print(f\"Train: {len(train_grouped['tokens'])} sequences of {SIZE} tokens each\")\n",
    "print(f\"Validation: {len(validation_grouped['tokens'])} sequences of {SIZE} tokens each\")\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_grouped)\n",
    "validation_dataset = Dataset.from_dict(validation_grouped)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset  # Using 'validation' as the standard name\n",
    "})\n",
    "dataset_dict.push_to_hub(f\"fax4ever/manzoni-{SIZE}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a56efa",
   "metadata": {},
   "source": [
    "Or we can simply load the result dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c18589",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(f\"fax4ever/manzoni-{SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4cce7",
   "metadata": {},
   "source": [
    "## PART TWO: Tokenize the dataset\n",
    "\n",
    "In the tokenization process each word of each input will become one or more tokens.\n",
    "First of all, we need to define some constants.\n",
    "In the constants we reflect the convention we implicitly found in the csv files. The 1 denotes the end and the beginning of a new sentence, while the 0 will be used to denote all the other tokens. Special tokens denoting start and end of the input-encoding sequences will be labeled with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_OF_SENTENCE = 1\n",
    "NOT_END_OF_SENTENCE = 0\n",
    "LABEL_FOR_START_END_OF_SEQUENCE = NOT_END_OF_SENTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c79d8",
   "metadata": {},
   "source": [
    "We need to choose a base embedding model used to classify the tokens as 0 or 1.\n",
    "Obviously, it means that we will use `num_labels=2`.\n",
    "\n",
    "Since the dataset is in Italian we would like to test models,\n",
    "that are multilingual or trained on an Italian corpus such as:\n",
    "\n",
    "1. üöÄ ModernBERT-base-ita (Most Recent - Dec 2024)\n",
    "  * `DeepMount00/ModernBERT-base-ita`\n",
    "\n",
    "2. üáÆüáπ Italian BERT XXL (Most Established): \n",
    "  * `dbmdz/bert-base-italian-xxl-cased`\n",
    "\n",
    "3. üåç XLM-RoBERTa (Best Multilingual)\n",
    "  * `FacebookAI/xlm-roberta-base`\n",
    "  * `FacebookAI/xlm-roberta-large`\n",
    "\n",
    "4. üî¨ Italian ELECTRA (Alternative Architecture): \n",
    "  * `dbmdz/electra-base-italian-xxl-cased-discriminator`\n",
    "\n",
    "Finally, since we're not going to classify the input texts but the tokens of the input text, we use the `AutoModelForTokenClassification` from the Hugging Face APIs, instead of `AutoModelForSequenceClassification`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d120457",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"bert-base-cased\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(EMBEDDING_MODEL, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049da04b",
   "metadata": {},
   "source": [
    "The original dataset provides labels for each `word`. When we tokenize the texts we need to align the labels to the generated tokens. \n",
    "The algorithm is pretty straightforward: we keep label 1 for all first tokens generated from a word marked as 1, we use 0 for all the other cases.\n",
    "With this strategy we keep the same number of 1s for each input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab24b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "def tokenize_and_align_labels(items):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        items[\"tokens\"], is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = items[\"labels\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = LABEL_FOR_START_END_OF_SEQUENCE if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        else:\n",
    "            # Treat the same word never as end of sentence\n",
    "            new_labels.append(NOT_END_OF_SENTENCE)\n",
    "    return new_labels\n",
    "\n",
    "tokenized_dataset_dict = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741f4c7",
   "metadata": {},
   "source": [
    "## PART THREE: Training\n",
    "\n",
    "The first thing to notice is the fact that the dataset is very unbalanced in terms of label distribution. Most of the labels are 0s and few 1s.\n",
    "In this case the accuracy is not a suitable metrics to measure the quality of our classifier.\n",
    "For instance returning always 0s will produce a high accuracy.\n",
    "\n",
    "The first measure we adopted was to use the f1 as metrics to select the best model we produced among all epoch models we produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as naming convention we use the embedding model name + \"-sentence-splitter\"\n",
    "# in this way we can easily identify the model we used to train the sentence splitter\n",
    "trained_model_name = EMBEDDING_MODEL + \"-sentence-splitter\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    trained_model_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    hub_token=os.environ['HF_TOKEN'],\n",
    "    load_best_model_at_end=True, # Stop training when F1 stops improving\n",
    "    metric_for_best_model=\"f1\" # Of course on the validation set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8b113",
   "metadata": {},
   "source": [
    "The second measure was to use a weighted cross entropy loss as loss function to be applied to the backpropagation.\n",
    "The function will count 30x more the errors in classification of 1s with respect to the ones on 0s. \n",
    "The factor (30) is another hyperparameter. In this case 30 comes from the fact that 96.7% of the labels have class 0 and 3.3% have the class 1.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model: nn.Module, inputs: dict[str, Union[torch.Tensor, Any]], return_outputs: bool = False, num_items_in_batch: Optional[torch.Tensor] = None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss for 2 labels with different weights\n",
    "        # Simple class weights: give sentence endings 30x more importance\n",
    "        # Based on your data: 96.7% class 0, 3.3% class 1 ‚Üí ~30:1 ratio\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 30.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        if num_items_in_batch is not None:\n",
    "            loss = loss / num_items_in_batch\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f53b9f",
   "metadata": {},
   "source": [
    "For each epoch we want to compute the metrics (precision, recall, f1 and accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded outside the compute_metrics function to avoid re-loading it at each epoch\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    all_metrics = metric.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdba42",
   "metadata": {},
   "source": [
    "Pushing all the epoch metrics and the final trained model to the Hugging Face hub. In this way the model produced can be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a290771",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = WeightedTrainer (\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset_dict[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_dict[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training complete\", token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc855b8f",
   "metadata": {},
   "source": [
    "## PART FOUR: Inference\n",
    "\n",
    "We define an inference pipeline using the model deployed on the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fe39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"fax4ever/\" + trained_model_name\n",
    "inference_pipeline = pipeline(\"token-classification\", model=model_checkpoint, \n",
    "                              aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92a4dc",
   "metadata": {},
   "source": [
    "And we pass any text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de28848",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Non era un legno di lusso, ma un semplice pezzo\n",
    "    da catasta, di quelli che d‚Äôinverno si mettono nelle\n",
    "    stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze.\n",
    "    Non so come andasse, ma il fatto gli √® che un bel\n",
    "    giorno questo pezzo di legno capit√≤ nella bottega\n",
    "    di un vecchio falegname, il quale aveva nome mastr‚ÄôAntonio, se non che tutti lo chiamavano maestro\n",
    "    Ciliegia, per via della punta del suo naso, che era\n",
    "    sempre lustra e paonazza, come una ciliegia matura.\n",
    "    Appena maestro Ciliegia ebbe visto quel pezzo di\n",
    "    legno, si rallegr√≤ tutto; e dandosi una fregatina di\n",
    "    mani per la contentezza, borbott√≤ a mezza voce:\n",
    "    \"Questo legno √® capitato a tempo; voglio servirmene per fare una gamba di tavolino.\" \n",
    "\"\"\"\n",
    "\n",
    "inference_pipeline(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

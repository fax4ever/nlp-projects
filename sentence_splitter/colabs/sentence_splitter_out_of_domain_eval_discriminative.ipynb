{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef88771",
   "metadata": {},
   "source": [
    "# Sentence Splitter: Out of Domain Evaluation\n",
    "\n",
    "## Evaluation Set Inference Using Generative Models\n",
    "\n",
    "In this notebook we will evaluate the models we produced for sentence splitting,\n",
    "on the same domain (Quarantana - Manzoni) and out of domain (on novels from different authors of the same time).\n",
    "We copied and slightly adapted the golden splitting novels from [Sentence Splitting Manzoni annotated dataset](https://github.com/RacheleSprugnoli/Sentence_Splitting_Manzoni/tree/main/test-novels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c3973",
   "metadata": {},
   "source": [
    "Install the libraries in the local virtual environment. \n",
    "We use specific versions to enforce reproducibility for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c58f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/fax/.local/lib/python3.13/site-packages (25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.56.1 in /home/fax/.local/lib/python3.13/site-packages (4.56.1)\n",
      "Requirement already satisfied: evaluate==0.4.5 in /home/fax/.local/lib/python3.13/site-packages (0.4.5)\n",
      "Requirement already satisfied: torch==2.7.0 in /home/fax/.local/lib/python3.13/site-packages (2.7.0)\n",
      "Requirement already satisfied: ipywidgets==8.1.7 in /home/fax/.local/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: scikit-learn==1.7.1 in /home/fax/.local/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.13/site-packages (from transformers==4.56.1) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/fax/.local/lib/python3.13/site-packages (from transformers==4.56.1) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib64/python3.13/site-packages (from transformers==4.56.1) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.13/site-packages (from transformers==4.56.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.13/site-packages (from transformers==4.56.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib64/python3.13/site-packages (from transformers==4.56.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (from transformers==4.56.1) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/fax/.local/lib/python3.13/site-packages (from transformers==4.56.1) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/fax/.local/lib/python3.13/site-packages (from transformers==4.56.1) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/lib/python3.13/site-packages (from transformers==4.56.1) (4.67.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/fax/.local/lib/python3.13/site-packages (from evaluate==0.4.5) (3.5.1)\n",
      "Requirement already satisfied: dill in /home/fax/.local/lib/python3.13/site-packages (from evaluate==0.4.5) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/lib64/python3.13/site-packages (from evaluate==0.4.5) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/fax/.local/lib/python3.13/site-packages (from evaluate==0.4.5) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/fax/.local/lib/python3.13/site-packages (from evaluate==0.4.5) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/fax/.local/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.5) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.13/site-packages (from torch==2.7.0) (74.1.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/lib/python3.13/site-packages (from torch==2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3.13/site-packages (from torch==2.7.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3.13/site-packages (from torch==2.7.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/fax/.local/lib/python3.13/site-packages (from torch==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/lib/python3.13/site-packages (from ipywidgets==8.1.7) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/lib/python3.13/site-packages (from ipywidgets==8.1.7) (8.36.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3.13/site-packages (from ipywidgets==8.1.7) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/fax/.local/lib/python3.13/site-packages (from ipywidgets==8.1.7) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/fax/.local/lib/python3.13/site-packages (from ipywidgets==8.1.7) (3.0.15)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/lib64/python3.13/site-packages (from scikit-learn==1.7.1) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/lib/python3.13/site-packages (from scikit-learn==1.7.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3.13/site-packages (from scikit-learn==1.7.1) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/fax/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.1) (1.1.9)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/lib64/python3.13/site-packages (from datasets>=2.0.0->evaluate==0.4.5) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/lib64/python3.13/site-packages (from datasets>=2.0.0->evaluate==0.4.5) (3.10.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.5) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.5) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.5) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/lib64/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.5) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/lib64/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.5) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/lib64/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.5) (1.13.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3.13/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate==0.4.5) (3.10)\n",
      "Requirement already satisfied: decorator in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets==8.1.7) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /usr/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.1.7) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.7) (0.8.4)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /usr/lib/python3.13/site-packages (from joblib>=1.2.0->scikit-learn==1.7.1) (3.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.7) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests->transformers==4.56.1) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests->transformers==4.56.1) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.13/site-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.13/site-packages (from pandas->evaluate==0.4.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.13/site-packages (from pandas->evaluate==0.4.5) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.5) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets==8.1.7) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets==8.1.7) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets==8.1.7) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers==4.56.1 evaluate==0.4.5 torch==2.7.0 ipywidgets==8.1.7 scikit-learn==1.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe62fc",
   "metadata": {},
   "source": [
    "Import the necessary libraries for model inference, data processing, and evaluation.\n",
    "We do this first to fail fast in case additional packages need to be installed in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fb30d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fax/code/nlp-projects/sentence_splitter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8e03e",
   "metadata": {},
   "source": [
    "Set up deterministic behavior for reproducible results by configuring random seeds for all relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397cfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed() # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e62fcc",
   "metadata": {},
   "source": [
    "List the fine-tuned (based on embedding models) models we want to test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9739f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"bert-base-cased-sentence-splitter\",\n",
    "    \"ModernBERT-base-ita-sentence-splitter\",\n",
    "    \"bert-base-italian-xxl-cased-sentence-splitter\",\n",
    "    \"xlm-roberta-base-sentence-splitter\",\n",
    "    \"xlm-roberta-large-sentence-splitter\",\n",
    "    \"electra-base-italian-xxl-cased-discriminator-sentence-splitter\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1c14c",
   "metadata": {},
   "source": [
    "### Data Preparation and Alignment\n",
    "\n",
    "We want to evaluate the f1 score of the splitting prediction compared.\n",
    "\n",
    "First of all, from a prediction and from a golden-annotated novel fragment we want to produce label.\n",
    "The labels produced for the evaluation here are different from the ones used to fine-tune the models:\n",
    "for simplicity of the implementation we have a label for each character instead of a label for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2142efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_eval(inference_pipeline, sequence_lines):\n",
    "    sequence = \" \".join(sequence_lines)\n",
    "    prediction = inference_pipeline(sequence)\n",
    "    prediction_labels = labels_from_prediction(prediction)\n",
    "    golden_labels = labels_from_novel(sequence_lines)\n",
    "\n",
    "    if (len(prediction_labels) < len(golden_labels)):\n",
    "        print(\"Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\")\n",
    "        golden_labels = golden_labels[:len(prediction_labels)]\n",
    "\n",
    "    return prediction_labels, golden_labels\n",
    "\n",
    "def labels_from_prediction(prediction):\n",
    "    ones = {}\n",
    "    for label in prediction:\n",
    "        if label[\"entity_group\"] == \"LABEL_1\":\n",
    "            for i in range(label[\"start\"], label[\"end\"]):\n",
    "                ones[i] = 1\n",
    "\n",
    "    first = prediction[0]\n",
    "    last = prediction[-1]\n",
    "    labels = []\n",
    "    for i in range(first[\"start\"], last[\"end\"]):\n",
    "        if i in ones:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels_from_novel(novel_lines):\n",
    "    labels = []\n",
    "    for i, novel_line in enumerate(novel_lines):\n",
    "        for _ in range(len(novel_line) - 1):\n",
    "            labels.append(0)\n",
    "        labels.append(1)\n",
    "        if i < len(novel_lines) - 1:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694a560",
   "metadata": {},
   "source": [
    "### Model Inference and Evaluation\n",
    "\n",
    "The golden-annotated novel is grouped in chunk of equal size of lines.\n",
    "\n",
    "From each chunk we evaluate the f1, comparing the labels generated from the golden annotated chunk,\n",
    "with the prediction made by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c37ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model_name: str, novel_path: Path, num_lines_for_eval: int):\n",
    "    model_checkpoint = \"fax4ever/\" + model_name\n",
    "    inference_pipeline = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
    "    novel_lines = novel_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "    return eval(inference_pipeline, novel_lines, num_lines_for_eval)\n",
    "\n",
    "def eval(inference_pipeline, novel_lines, num_lines_for_eval):\n",
    "    metric = evaluate.load(\"f1\", average=\"binary\")\n",
    "\n",
    "    grouped_lines = np.array_split(novel_lines, groups(novel_lines, num_lines_for_eval))\n",
    "    for sequence_lines in grouped_lines:\n",
    "        prediction_labels, golden_labels = sequence_eval(inference_pipeline, sequence_lines)\n",
    "        metric.add_batch(predictions=prediction_labels, references=golden_labels)\n",
    "\n",
    "    return metric.compute()  \n",
    "\n",
    "def groups(novel_lines, num_lines_for_eval):\n",
    "    groups = len(novel_lines) / num_lines_for_eval\n",
    "    if len(novel_lines) % num_lines_for_eval != 0:\n",
    "        groups += 1\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950f2c6",
   "metadata": {},
   "source": [
    "We test in-domain novel (Quarantana) and 3 novels from different authors of the same period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e8b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased-sentence-splitter Cuore-GOLD.txt 0.9364161849710982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased-sentence-splitter Malavoglia-GOLD.txt 0.782608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased-sentence-splitter Pinocchio-GOLD.txt 0.8854166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\n",
      "Truncating golden labels. You should use a smaller value for NUM_LINES_FOR_EVAL!\n",
      "bert-base-cased-sentence-splitter Quarantana-GOLD.txt 0.9859154929577465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Cuore-GOLD.txt 0.8144329896907216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Malavoglia-GOLD.txt 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Pinocchio-GOLD.txt 0.5603112840466926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBERT-base-ita-sentence-splitter Quarantana-GOLD.txt 0.9130434782608695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Cuore-GOLD.txt 0.9485714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Malavoglia-GOLD.txt 0.9120879120879121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Pinocchio-GOLD.txt 0.8773584905660378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-italian-xxl-cased-sentence-splitter Quarantana-GOLD.txt 0.9938271604938271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Cuore-GOLD.txt 0.24242424242424243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Malavoglia-GOLD.txt 0.21153846153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Pinocchio-GOLD.txt 0.5426356589147286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base-sentence-splitter Quarantana-GOLD.txt 0.3952380952380952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Cuore-GOLD.txt 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Malavoglia-GOLD.txt 0.3559322033898305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Pinocchio-GOLD.txt 0.6618705035971223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large-sentence-splitter Quarantana-GOLD.txt 0.5274725274725275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Cuore-GOLD.txt 0.9385474860335196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Malavoglia-GOLD.txt 0.9347826086956522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Pinocchio-GOLD.txt 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra-base-italian-xxl-cased-discriminator-sentence-splitter Quarantana-GOLD.txt 0.9938461538461538\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    novel = \"Cuore-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 6)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Malavoglia-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 3)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Pinocchio-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 7)\n",
    "    print(model_name, novel, value[\"f1\"])\n",
    "\n",
    "    novel = \"Quarantana-GOLD.txt\"\n",
    "    value = eval_model(model_name, Path('out_of_domain_data') / novel, 5)\n",
    "    print(model_name, novel, value[\"f1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

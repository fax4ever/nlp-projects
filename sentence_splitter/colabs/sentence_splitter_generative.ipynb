{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26351d9",
   "metadata": {},
   "source": [
    "# Sentence Splitter using an LLM\n",
    "\n",
    "Install the required libraries in the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch numpy pandas datasets jupyter unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fdfa7",
   "metadata": {},
   "source": [
    "Let's import everything we need:\n",
    "\n",
    "(doing it at the beginning to fail fast in case we need something else to install in our virtual environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef295e",
   "metadata": {},
   "source": [
    "First of all, let's verify we support accelerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2635b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9aa7a",
   "metadata": {},
   "source": [
    "Before doing everything else try to make this run as much as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed() # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fef26b",
   "metadata": {},
   "source": [
    "## PART ONE: Create the dataset\n",
    "\n",
    "For the LLM part of the project we can start from the dataset we already created for the embedding part:\n",
    "[fax4ever/manzoni-192](https://huggingface.co/datasets/fax4ever/manzoni-192).\n",
    "\n",
    "To see how this dataset is created from the `CSV` files, see the `colabs/sentence_splitter_embeddings.ipynb` notebook.\n",
    "\n",
    "In this setting we don't need labels for each words, but we need to have conversations to train and to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 192 # Number of words to put on each input of the encoder model\n",
    "\n",
    "def create_conversations(examples):\n",
    "    input_texts = []\n",
    "    output_texts = []\n",
    "\n",
    "    for tokens, labels in zip(examples['tokens'], examples['labels']):\n",
    "        input_text = \" \".join(tokens)\n",
    "        input_texts.append(input_text)\n",
    "\n",
    "        sentences = []\n",
    "        current_sentence = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            current_sentence.append(token)\n",
    "            if label == 1:  # End of sentence\n",
    "                sentences.append(\" \".join(current_sentence).strip())\n",
    "                current_sentence = []\n",
    "        \n",
    "        # Add remaining tokens if any\n",
    "        if current_sentence:\n",
    "            sentences.append(\" \".join(current_sentence).strip())\n",
    "\n",
    "        output_text = \"\\n\".join([f\"{i+1}. {sentence}\" for i, sentence in enumerate(sentences)])\n",
    "        output_texts.append(output_text)\n",
    "\n",
    "    return {\"input_text\" : input_texts, \"output_text\" : output_texts}\n",
    "\n",
    "dataset_dict = load_dataset(f\"fax4ever/manzoni-{SIZE}\")\n",
    "llm_dataset_dict = dataset_dict.map(create_conversations, batched = True)\n",
    "llm_dataset_dict.push_to_hub(f\"fax4ever/llm-manzoni-{SIZE}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91bb06",
   "metadata": {},
   "source": [
    "The result is published as a Hugging Face dataset, so standard Hugging Face API could be applied on it.\n",
    "That is the benefit of following an open standard!\n",
    "Conversations here are expressed in terms of questions (input_text) and answers (output_text).\n",
    "\n",
    "Again, the result is published as a Hugging Face dataset, so standard Hugging Face API could be applied on it.\n",
    "That is the benefit of following an open standard!\n",
    "\n",
    "Or we can simply load the result dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c263fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(f\"fax4ever/llm-manzoni-{SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d0810",
   "metadata": {},
   "source": [
    "## PART TWO: Create the prompts\n",
    "\n",
    "In this phase we're going to create prompts from the series of questions / answers we have in the dataset.\n",
    "Following an object oriented approach, we define a class to produce each prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b391890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, input_text):\n",
    "        self.input_text = input_text\n",
    "\n",
    "    def instruction(self):\n",
    "        return f\"\"\"Dividi il seguente testo italiano in frasi. Per favore rispondi con una frase per riga. Grazie.\n",
    "\n",
    "Testo: {self.input_text}\n",
    "\"\"\"\n",
    "\n",
    "    def conversation(self, output_text):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "            {\"role\" : \"assistant\", \"content\" : output_text},\n",
    "        ]\n",
    "\n",
    "    def question(self):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd0d23",
   "metadata": {},
   "source": [
    "The `conversation` method will produce a full question / answer converation, that will be used for training.\n",
    "the `question` method will produce just a question prompt, that will be use for inference."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

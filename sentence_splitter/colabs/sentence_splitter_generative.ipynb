{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26351d9",
   "metadata": {},
   "source": [
    "# Sentence splitter using an LLM\n",
    "\n",
    "Install the required libraries in your virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch numpy pandas datasets jupyter unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fdfa7",
   "metadata": {},
   "source": [
    "Import all required libraries.\n",
    "\n",
    "We do this first to fail fast in case additional packages need to be installed in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef295e",
   "metadata": {},
   "source": [
    "Verify that a hardware accelerator is available.\n",
    "\n",
    "This notebook requires a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2635b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = 'PUT_YOUR_TOKEN_HERE'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9aa7a",
   "metadata": {},
   "source": [
    "Before proceeding, make the run as deterministic as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 777\n",
    "\n",
    "def set_seed(seed=777, total_determinism=False):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if total_determinism:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_STATE) # Set the seed for reproducibility -- use_deterministic_algorithms can make training slower :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fef26b",
   "metadata": {},
   "source": [
    "## Part one: Create the dataset\n",
    "\n",
    "For the LLM portion of the project, start from the dataset already created for the embedding model: [fax4ever/manzoni-192](https://huggingface.co/datasets/fax4ever/manzoni-192).\n",
    "\n",
    "To see how this dataset is built from the CSV files, refer to `colabs/sentence_splitter_embeddings.ipynb`.\n",
    "\n",
    "In this setting we do not need labels for each word; instead, we need conversations for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 192 # Number of words to put on each input of the encoder model\n",
    "\n",
    "def words_to_sentences(words):\n",
    "    input_text = \" \".join(words)\n",
    "    input_text = input_text.replace(\" ,\", \",\")\n",
    "    input_text = input_text.replace(\" .\", \".\")\n",
    "    input_text = input_text.replace(\" ?\", \"?\")\n",
    "    input_text = input_text.replace(\" !\", \"!\")\n",
    "    input_text = input_text.replace(\" :\", \":\")\n",
    "    input_text = input_text.replace(\" ;\", \";\")\n",
    "    input_text = input_text.replace(\"' \", \"'\")\n",
    "    return input_text\n",
    "\n",
    "def create_conversations(examples):\n",
    "    input_texts = []\n",
    "    output_texts = []\n",
    "\n",
    "    for tokens, labels in zip(examples['tokens'], examples['labels']):\n",
    "        input_text = words_to_sentences(tokens)\n",
    "        input_texts.append(input_text)\n",
    "\n",
    "        sentences = []\n",
    "        current_sentence = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            current_sentence.append(token)\n",
    "            if label == 1:  # End of sentence\n",
    "                sentences.append(words_to_sentences(current_sentence))\n",
    "                current_sentence = []\n",
    "\n",
    "        if current_sentence:\n",
    "            sentences.append(words_to_sentences(current_sentence))\n",
    "\n",
    "        output_text = \"\\n\".join([f\"{i+1}. {sentence}\" for i, sentence in enumerate(sentences)])\n",
    "        output_texts.append(output_text)\n",
    "\n",
    "    return {\"input_text\" : input_texts, \"output_text\" : output_texts}\n",
    "\n",
    "dataset_dict = load_dataset(f\"fax4ever/manzoni-{SIZE}\")\n",
    "llm_dataset_dict = dataset_dict.map(create_conversations, batched = True)\n",
    "llm_dataset_dict.push_to_hub(f\"fax4ever/llm-manzoni-{SIZE}\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91bb06",
   "metadata": {},
   "source": [
    "The result is published as a Hugging Face dataset, so standard Hugging Face APIs apply.\n",
    "\n",
    "Conversations are expressed as questions (`input_text`) and answers (`output_text`).\n",
    "\n",
    "Alternatively, simply load the dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c263fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dataset_dict = load_dataset(f\"fax4ever/llm-manzoni-{SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d0810",
   "metadata": {},
   "source": [
    "## Part two: Create the prompts\n",
    "\n",
    "In this phase we create prompts from the question/answer pairs in the dataset.\n",
    "Following an object-oriented approach, we define a class to produce each prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b391890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, input_text):\n",
    "        self.input_text = input_text\n",
    "\n",
    "    def instruction(self):\n",
    "        return f\"\"\"Dividi il seguente testo italiano in frasi. Per favore rispondi con una frase per riga. Grazie.\n",
    "\n",
    "Testo: {self.input_text}\n",
    "\"\"\"\n",
    "\n",
    "    def conversation(self, output_text):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "            {\"role\" : \"assistant\", \"content\" : output_text},\n",
    "        ]\n",
    "\n",
    "    def question(self):\n",
    "        return[\n",
    "            {\"role\" : \"system\",    \"content\" : \"Sei un esperto di linguistica italiana specializzato nella segmentazione delle frasi.\"},\n",
    "            {\"role\" : \"user\",      \"content\" : self.instruction()},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd0d23",
   "metadata": {},
   "source": [
    "The `conversation` method produces a full question/answer conversation and is used to fine‑tune the model.\n",
    "The `question` method produces only the question prompt and will be used for inference later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversations(examples):\n",
    "    input_texts  = examples[\"input_text\"]\n",
    "    output_texts = examples[\"output_text\"]\n",
    "\n",
    "    conversations = []\n",
    "    for input_text, output_text in zip(input_texts, output_texts):\n",
    "        conversations.append(Prompt(input_text).conversation(output_text))\n",
    "    return { \"conversations\": conversations, }\n",
    "\n",
    "\n",
    "conversations = llm_dataset_dict.map(create_conversations, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008379c9",
   "metadata": {},
   "source": [
    "## Part three: Parameter‑efficient fine‑tuning\n",
    "\n",
    "We define a quantized model and then apply a LoRA (Low‑Rank Adaptation) adapter\n",
    "to enable fine‑tuning the LLM with modest resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"unsloth/Qwen3-4B\"\n",
    "BASE_MODEL_NAME = \"Qwen3-4B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = LLM_MODEL,  # you can use the 14B here!\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = RANDOM_STATE,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c2e1a",
   "metadata": {},
   "source": [
    "We need to convert the conversation templates into the canonical format for this model.\n",
    "We will use the model’s tokenizer to do this.\n",
    "From this, we will create the final dataset used for supervised fine‑tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dataset = conversations.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"conversations\"], tokenize=False)})\n",
    "\n",
    "train_formatted_chats = pd.Series(chat_dataset['train']['formatted_chat'])\n",
    "train_formatted_chats.name = \"text\"\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_formatted_chats))\n",
    "\n",
    "validation_formatted_chats = pd.Series(chat_dataset['validation']['formatted_chat'])\n",
    "validation_formatted_chats.name = \"text\"\n",
    "validation_dataset = Dataset.from_pandas(pd.DataFrame(train_formatted_chats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d1ee6",
   "metadata": {},
   "source": [
    "Finally, train the model and save it remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = validation_dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,  # ~500-2000 or 10-20% of the total steps\n",
    "        num_train_epochs = 10,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = BASE_MODEL_NAME + \"-sentence-splitter\"\n",
    "model_checkpoint = \"fax4ever/\" + trained_model_name\n",
    "\n",
    "model.push_to_hub(model_checkpoint, token=os.environ['HF_TOKEN'])\n",
    "tokenizer.push_to_hub(model_checkpoint, token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32089c83",
   "metadata": {},
   "source": [
    "## Part four: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Non era un legno di lusso, ma un semplice pezzo\n",
    "da catasta, di quelli che d’inverno si mettono nelle\n",
    "stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze.\n",
    "Non so come andasse, ma il fatto gli è che un bel\n",
    "giorno questo pezzo di legno capitò nella bottega\n",
    "di un vecchio falegname, il quale aveva nome mastr’Antonio, se non che tutti lo chiamavano maestro\n",
    "Ciliegia, per via della punta del suo naso, che era\n",
    "sempre lustra e paonazza, come una ciliegia matura.\n",
    "Appena maestro Ciliegia ebbe visto quel pezzo di\n",
    "legno, si rallegrò tutto; e dandosi una fregatina di\n",
    "mani per la contentezza, borbottò a mezza voce:\n",
    "\"Questo legno è capitato a tempo; voglio servirmene per fare una gamba di tavolino.\" \n",
    "\"\"\"\n",
    "input_text = input_text.splitlines()\n",
    "input_text = \" \".join(input_text)\n",
    "\n",
    "question = tokenizer.apply_chat_template(\n",
    "    [Prompt(input_text).question()], \n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer(question, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

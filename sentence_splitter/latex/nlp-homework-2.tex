\pdfoutput=1
\documentclass[11pt]{article}

\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{listings}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{%
	Sentence Splitter\\
	\large Multilingual Natural Language Processing \\
	Homework 2}
\date{August $10^{th}$, 2025}

\author{Ercoli Fabio Massimo \\
\texttt{802397} \\\And
Della Porta Nicolò \\
\texttt{1920468} \\}

\begin{document}

\maketitle

\section{Introduction}

Quoting \cite{redaelli-sprugnoli-2024-sentence} "Sentence splitting, that is the segmentation of the raw input text into sentences, is a fundamental step in text processing".  According to \cite{frohmann2024segmenttextuniversalapproach} the main challenges are:

\begin{itemize}
	\item Robustness to missing punctuation
	\item Effective adaptability to new domains
	\item High efficiency
\end{itemize}

According to \cite{redaelli-sprugnoli-2024-sentence} we can add to the list:

\begin{itemize}
	\item Multilinguality
\end{itemize}

A sentence splitting approach that works well for English may not work well to split another language.

In this project we implemented two types of models for sentence splitting, using an Italian corpus as train and validation set.
The first kind is based on an embedding model, the second is based on a generative LLM.
We aim to analyze and compare the two approaches. 
We also plan to test the models out of the original domain of training, that is an annotated text from I Promessi Sposi (Quarantana) by Manzoni.

\section{Methodology}

\subsection{Embedding-based}

We fine-tuned a pretrained embedding model using the train and validation datasets provided by the homework guide.

The original dataset provides two large texts (one to use as train and the other as validation) together with the golden labels to mark the end of sentence (1) and all the rest of the words (0).

In order to make the datasets suitable for training, we had to apply some transformations. 
We needed to group words into sequences, and to tokenize words into tokens, aligning the golden labels consistently.
With the constraint that the number of tokens of each sequence must fit the max length of the embedding models,
for instance 512.

We generated a dataset using sequences of 192 words each. Notice that the number of tokens for each sequence will be strictly greater,
since for each word the tokenizer of the model will produce one or more tokens.

Before using the datasets, we still need to align (as mentioned above) the labels to the tokens.
The alignment strategy we applied consists of keeping 1 as the first token generated from a word
having label 1, and using 0 for all the other cases.

One aspect that we had to address was the fact that the label distribution is very unbalanced
for this use case. Most of the labels are 0s, and few are 1s. We applied 2 different strategies.

First, we set the \textbf{load\_best\_model\_at\_end} training argument to \textbf{true},
using \textbf{metric\_for\_best\_model} set to F1. This implements early stopping based on F1,
rather than on accuracy (which would be misleading for such an unbalanced dataset).

Second, we overrode the loss function to weight misclassification of class 1 (end of sentence) 30× more than class 0. 
This weighting factor was motivated by the label distribution (96.7\% class 0 vs 3.3\% class 1). 
We implemented this using a custom weighted trainer. 

During inference, we don't need to align the labels, since the labels are generated by the model.

\subsection{LLM-based}

Fine-tuning an LLM is usually prohibitive or impossible using modest hardware, such as a single GPU with 16 GB / 24 GB RAM. So we applied two techniques to reduce the amount of memory required (and also the time) to fine-tune the models: LoRA (Low-Rank Adaptation) and parameter quantization.

A special focus was given to creating good prompts for both the training and inference phases.
We first generated new datasets from the ones we used for the embedding-based solution, in which 
for every sequence we have:

 \begin{itemize}
 	\item \emph{input\_text}: here we simply concatenate all words of the sequence into a single text
 	\item \emph{output\_text}: here we group all the words that belong to the same sentence on different lines, numbered sequentially
 \end{itemize}
 
 In the process also we add the spaces between the words, paying attention to
 not introduce a space before a punctuation sign or after an apostrophe.
 
 This is not a prompt yet. To create the final prompts, we created dictionaries of roles (system / user / assistant) with contents (we called them conversations) and passed them to the LLM tokenizer in order to have them converted into the proper chat templates. 
 
 We used the Supervised Fine-Tuning (TRL SFT) Trainer to train the model.

The fine-tuned models are (as usual) pushed to the Hugging Face repository, so that they can be used for inference from anywhere.

\section{Experiments}

\subsection{Embedding-based: Training Evaluation}

The performance goal we set is to maximize the F1 score on the validation set. Our expectation is that a good result on the
validation set can generalize nicely on the test set.

For the embedding-based solution, we tested the following base models:

 \begin{itemize}
 	\item BERT base cased
	\item ModernBERT-base-ita (Dec 2024)
	\item Italian BERT XXL (most established)
	\item XLM-RoBERTa base
	\item XLM-RoBERTa large
	\item Italian ELECTRA
\end{itemize}

For the LLM-based solution, we tested the following base models:

 \begin{itemize}
 	\item Qwen3 4B
	\item Meta Llama 3.1 8B Instruct
	\item Mistral 7B Instruct v0.3 bnb 4bit
	\item Minerva 7B Instruct v1.0
\end{itemize}

As we said, both test and validation set come from the same novel: \emph{I Promessi Sposi}. 

\subsection{Embedding-based: Out of Domain Evaluation}

We designed experiments to test the same models on different authors.
For this purpose, we planned to use the dataset from \cite{redaelli-sprugnoli-2024-sentence}, in which different authors
from the same century are used.
The dataset has already the gold results annotated by the authors of the paper.

We evaluated the f1 score of the predictions, having the annotated
novels as ground truth.

We produced labels from both the annotated novels and 
from predictions. For simplicity of the implementation,
for this phase, we produce a label for each character, instead of a label for each token.

The golden-annotated novel rows were grouped in chunks of equal size of lines, to produce sequences.

We tested the in-domain novel (Quarantana) and 3 novels (Pinocchio, I Malavoglia and Cuore) from different authors of the same period.

\subsection{LLM-based}

Results for LLM-based (generative)  models will be evaluated manually, simply taking an out of domain 
text fragment from Pinocchio novel and pass it as input to the target test models.

\section{Results}

\subsection{Embedding-based: Training Evaluation}

\begin{table}[]
	\small
	\caption{Best F1-epoch Embedding-based}
	\begin{tabular}{llll}
		Models & Best Epoch & F1 & Valid loss \\
		BERT base cased & 6      & 0.9922        & 0.0014                  \\
		ModernBERT base ITA  & 12      & 0.9938        & 0.0097                  \\
		BERT base ITA XXL cased & 11      & 0.9922        & 0.0021                 \\
		XLM RoBERTa base & 4       & 0.9953        & 0.0014                  \\
		XLM RoBERTa large & 12       & 0.9953       & 0.0087                 \\
		Electra ITA XXL disc. & 20       & 0.9953        & 0.0015                
	\end{tabular}
	\label{t1}
\end{table}

Table \ref{t1} reports the F1 scores and the losses on validation set 
for the best-F1 epoch, training the network with 30 epochs.
This is of course an in-domain evaluation, since the validation set
is from the same author (Manzoni) and even also the same novel
(I Promessi Sposi).
On training, the F1s on the validation set look very good.

\subsection{Embedding-based: Out of Domain Evaluation}

\begin{table}[]
	\small
	\caption{Out-Of-Domain Evaluation: Embedding-based}
	\begin{tabular}{lllll}
		Models & Quara. & Pinocc. & Malav. & Cuore \\
		BERT base cased & 0.9922      & 0.8058 & 0.7304        & 0.8757                  \\
		ModernBERT ITA  & 0.9675     & 0.8495   & 0.8128   & 0.8864                  \\
		BERT ITA XXL  & 0.9938      & 0.9947   & 0.9205     & 0.9591                 \\
		XLM RoBERTa B & 0.3953       & 0.5427   & 0.1980     & 0.2424                 \\
		XLM RoBERTa L & 0.5275       & 0.6618    & 0.3559   & 0.5               \\
		Electra ITA XXL  & 0.9923       & 0.9785    & 0.9171    & 0.9655             
	\end{tabular}
	\label{t2}
\end{table}

When we evaluate the same models on different novels we got the
result reported in table \ref{t2}.

According to our tests, multilingual base models (XLM RoBERTa base and large)
got very bad results, even in the same domain (Quarantana).

It seems to be better to use BERT base, even if it is not trained on Italian.
This base model, like ModernBERT ITA, works very good in domain (Quarantana) and
decently out of domain (other novels).

BERT ITA XXL and Electra ITA XXL got great results both in domain and out of domain.

The novel that seems to be the most difficult to split for all models is I Malavoglia.
Best result is usually achieved in domain (as expected).

\subsection{LLM-based}

All the 4 LLM-based tested models provide the same output to the text 
we passed to the prompt. The output text is reported on Appendix \ref{sec:appendix1}
and looks totally correct. 

Unfortunately, even the inference requires a GPU and also 
you can usually use only one model on the same notebook run.

The models can hallucinate. For instance, we experienced in some runs that 
sometimes characters, in particular white spaces had been fabricated 
out of thin air
by the model: they were not present in the input text to split.

\subsection{Conclusions}

The embedding base models are very efficient (they usually require a
number of parameter at least 1 order magnitude less than the LLM-based ones).
Also if a good base Italian model is chosen (for instance BERT ITA XXL)
results are great both in domain and out of domain.

The LLM-based ones also seem to provide good results in terms of quality, but they can hallucinate, while it is simply not possible using an embedding-based approach.

Moreover, the latter are much more heavy to train and to use at inference time.

With the only exception of qwen3-4b-unsloth-bnb-4bit that takes 5 minutes
to be trained the other take from 40 minutes to 1 hour to be trained,
on a max memory GPU of 22.069 GB.

Furthermore,
we had to use LoRA and the parameter quantization in order to train the models
on a modest hardware.

All in all, the team has thinks that using an embedding based model
would be better the natural choice to address the sentence splitting use case.

\appendix

\section{LLM output Appendix}
\label{sec:appendix1}

The sentence splitting output returned by the LLM-based model is:

{\fontfamily{sffamily}\selectfont

\vspace{3mm}

\noindent 1. Non era un legno di lusso, ma un semplice pezzo da catasta, di quelli che d’inverno si mettono nelle stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze.

\noindent 2. Non so come andasse, ma il fatto gli è che un bel giorno questo pezzo di legno capitò nella bottega di un vecchio falegname, il quale aveva nome mastr’Antonio, se non che tutti lo chiamavano maestro Ciliegia, per via della punta del suo naso, che era sempre lustra e paonazza, come una ciliegia matura.

\noindent 3. Appena maestro Ciliegia ebbe visto quel pezzo di legno, si rallegrò tutto; e dandosi una fregatina di mani per la contentezza, borbottò a mezza voce: "Questo legno è capitato a tempo; voglio servirmene per fare una gamba di tavolino."

\vspace{3mm}

}

The Minerva-based and the Llama-based model also returned the token \emph{<|eot\_id|>}
at the very end. While the Mistral-based model appended \emph{</s>} at the end.
The Quen3-based added \emph{<|im\_end|>} at the end.

\section{Notebook Descriptions Appendix}
\label{sec:appendix2}

\subsection{Notebook  List}

We have produced 4 notebooks:

 \begin{itemize}
	\item \textbf{sentence\_splitter\_embeddings.ipynb}: it trains an embedding-based
	model.
	\item \textbf{sentence\_splitter\_generative.ipynb}: it trains a LLM-based model.
	\item \textbf{sentence\_splitter\_out\_of\_domain\_eval.ipynb}: it performs the out of domain 
	evaluations for (already deployed) embedding-based models.
	\item \textbf{sentence\_splitter\_generative\_eval.ipynb}: it tests a single splitting on
	LLM-based models.
\end{itemize}

\subsection{Google Colab Execution}

The notebooks have been developed to be run on a \href{https://jupyterlab.readthedocs.io/en/latest/#}{JupiterLab}
environment. 

To run those on  Google Colab, you should replace:

\begin{verbatim}
	os.environ['HF_TOKEN']
\end{verbatim}

with

\begin{verbatim}
	userdata.get('HF_TOKEN')
\end{verbatim}

and also you need to add the following line to the input:

\begin{verbatim}
	from google.colab import userdata
\end{verbatim}

Moreover, if you want to run the trainings part, you need to change 
the Hugging Face repository (in the notebook \emph{fax4ever}),
providing consistently your Hugging Face token.

While you don't need to set the Hugging Face token to
run the inference notebooks.

\section{Models and Datasets Appendix}
\label{sec:appendix3}

All the fine tuned models and the datasets were published to Hugging Face 
hub.

\subsection{Embedding-based released models}

 \begin{itemize}
	\item \href{https://huggingface.co/fax4ever/bert-base-cased-sentence-splitter}{bert-base-cased-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/ModernBERT-base-ita-sentence-splitter}{ModernBERT-base-ita-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/bert-base-italian-xxl-cased-sentence-splitter}{bert-base-italian-xxl-cased-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/xlm-roberta-base-sentence-splitter}{xlm-roberta-base-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/xlm-roberta-large-sentence-splitter}{xlm-roberta-large-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/electra-base-italian-xxl-cased-discriminator-sentence-splitter}{electra-base-italian-xxl-cased-discriminator-sentence-splitter}
\end{itemize}

\subsection{LLM-based released models}

\begin{itemize}
	\item 
	\href{https://huggingface.co/fax4ever/qwen3-4b-unsloth-bnb-4bit-sentence-splitter}{qwen3-4b-unsloth-bnb-4bit-sentence-splitter}
	\item 
	\href{https://huggingface.co/fax4ever/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit-sentence-splitter}{meta-llama-3.1-8b-instruct-unsloth-bnb-4bit-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/mistral-7b-instruct-v0.3-bnb-4bit-sentence-splitter}{mistral-7b-instruct-v0.3-bnb-4bit-sentence-splitter}
	\item \href{https://huggingface.co/fax4ever/Minerva-7B-instruct-v1.0-sentence-splitter}{Minerva-7B-instruct-v1.0-sentence-splitter}
\end{itemize}

\subsection{Datasets}

The main Dataset to train the embedding-based models and LLM-based model are:

\begin{itemize}
	\item \href{https://huggingface.co/datasets/fax4ever/manzoni-192}{manzoni-192}
	\item \href{https://huggingface.co/datasets/fax4ever/llm-manzoni-192}{llm-manzoni-192}
\end{itemize}

Those are strictly derived from the CSVs files that were provided to do the Homework.
No other data then that was used to train the models.

\section{Indeterminism Appendix}
\label{sec:appendix3}

We tried to make the runs as deterministic as possible, but without affecting the
performance.
It means that some output derived from indeterministic algorithms may change with the executions.
Making all deterministic would have affected too much the performance.
So, it means that if you run everything from scratch, you could find slightly different results.

For instance the second time we re-trained everything from scratch we got different results
for the out of domain evaluations, see Table \ref{t3}.

\begin{table}[]
	\small
	\caption{Out-Of-Domain Evaluation: Embedding-based (TAKE-2!)}
	\begin{tabular}{lllll}
		Models & Quara. & Pinocc. & Malav. & Cuore \\
		BERT base cased & 0.9859      & 0.8854 & 0.7826        & 0.9364                 \\
		ModernBERT ITA  & 0.9130     & 0.5603   & 0.6667   & 0.8144                 \\
		BERT ITA XXL  & 0.9939     & 0.8774  & 0.9121     & 0.9486                 \\
		XLM RoBERTa B & 0.3952      & 0.5426   & 0.2115     & 0.2424                 \\
		XLM RoBERTa L & 0.5275       & 0.6618    & 0.3559   & 0.5               \\
		Electra ITA XXL  & 0.9938      & 1.0   & 0.9348    & 0.9385            
	\end{tabular}
	\label{t3}
\end{table}

Anyway, we can confirm the same analysis we made on the take-1.
That's the hard life of the data scientists, even in the Deep Learning era!

\bibliography{custom}

\end{document}


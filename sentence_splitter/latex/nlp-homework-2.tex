\pdfoutput=1
\documentclass[11pt]{article}

\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{listings}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{%
	Sentence Splitter\\
	\large Multilingual Natural Language Processing \\
	Homework 2}
\date{August $10^{th}$, 2025}

\author{Ercoli Fabio Massimo \\
\texttt{802397} \\\And
Della Porta Nicolò \\
\texttt{1920468} \\}

\begin{document}

\maketitle

\section{Introduction}

Quoting \cite{redaelli-sprugnoli-2024-sentence} "Sentence splitting, that is the segmentation of the raw input text into sentences, is a fundamental step in text processing".  According to \cite{frohmann2024segmenttextuniversalapproach} the main challenges are:

\begin{itemize}
	\item robustness to missing punctuation
	\item effective adaptability to new domains
	\item high efficiency
\end{itemize}

According to \cite{redaelli-sprugnoli-2024-sentence} we can add to the list:

\begin{itemize}
	\item multilinguality
\end{itemize}

Because a sentence splitting that works well for English may not work well to split another language.

In this project we implemented two models for sentence splitting, using an Italian corpus as train and validation set.
The first one is based on an embedding model, the second one is based on a generative LLM.
We aim to analyze and compare the two approaches. 
We also plan to test the models out-of-domain.

\section{Methodology}

\subsection{Embedding-based}

We fine-tuned a pretrained embedding model using the train and validation datasets provided by the homework guide.

The original dataset provides two large texts (one to use as train and the other as validation) together with the golden labels to mark the end of sentence (1) and all the rest of the words (0).

In order to make the datasets suitable for training, we had to apply some transformations. 
We needed to group words into sequences of tokens, aligning the golden labels consistently.
The number of tokens of each sequence must fit the max length of the embedding models,
for instance 512.

We generated a dataset using sequences of 192 words each. Notice that the number of tokens for each sequence will be strictly greater,
since for each word the tokenizer of the model will produce one or more tokens.

Before using the datasets, we still need to align (as mentioned above) the labels to the tokens.
The alignment strategy we applied consists of keeping 1 as the first token generated from a word
having label 1, and using 0 for all the other cases.

One aspect that we had to address was the fact that the label distribution is very unbalanced
for this use case. Most of the labels are 0s, and few are 1s. We applied 2 different strategies.

First, we set the \emph{load\_best\_model\_at\_end} training argument to \emph{true},
using \emph{metric\_for\_best\_model} set to F1. This implements early stopping based on F1,
rather than on accuracy (which would be misleading for such an unbalanced dataset).

Second, we overrode the loss function to weight misclassification of class 1 (end of sentence) 30× more than class 0. 
This weighting factor was motivated by the label distribution (96.7\% class 0 vs 3.3\% class 1). 
We implemented this using a custom weighted trainer. 

During inference, we don't need to align the labels, since the labels are generated by the model.

\subsection{LLM-based}

Fine-tuning an LLM is usually prohibitive or impossible using modest hardware, such as a single GPU with 8 GB / 16 GB RAM. So we applied two techniques to reduce the amount of memory required (and also the time) to fine-tune the models: LoRA (Low-Rank Adaptation) and parameter quantization.

A special focus was given to creating good prompts for both the training and inference phases.
We first generated new datasets from the ones we used for the embedding-based solution, in which 
for every sequence we have:

 \begin{itemize}
 	\item \emph{input\_text}: here we simply concatenate all words of the sequence into a single text
 	\item \emph{output\_text}: here we group all the words that belong to the same sentence on different lines, numbered sequentially
 \end{itemize}
 
 This is not a prompt yet. To create the final prompts, we created dictionaries of roles (system / user / assistant) with contents (we called them conversations) and passed them to the LLM tokenizer in order to have them converted into the proper chat templates. 
 
 We used the Supervised Fine-Tuning (TRL SFT) Trainer to train the model.

The fine-tuned model is (as usual) pushed to the Hugging Face repository, so that it can be used
for inference from anywhere.

\section{Experiments}

\subsection{F1 on different models - same domain}

The performance goal we set is to maximize the F1 score on the validation set. The test set is unlabeled, so we cannot use it. Our expectation is that with a good result on the
validation set can generalize nicely on the test set.

For the embedding-based solution, we implemented the approach using BERT-base-cased as the base model.
While we considered several Italian and multilingual models during development:

 \begin{itemize}
	\item ModernBERT-base-ita (Dec 2024)
	\item Italian BERT XXL (most established)
	\item XLM-RoBERTa base
	\item XLM-RoBERTa large
	\item Italian ELECTRA
\end{itemize}

The actual implementation uses BERT-base-cased for demonstration purposes.

For the LLM-based solution, we implemented the approach using Qwen3-4B Instruct.
We considered several instruction-tuned models during development:

 \begin{itemize}
	\item LLaMA-3-8B Instruct
	\item Mixtral 8x7B Instruct
	\item Qwen3-4B Instruct
\end{itemize}

The actual implementation uses Qwen3-4B Instruct (via unsloth/Qwen3-4B) with 4-bit quantization and LoRA fine-tuning.

These tests are executed on the same domain, since all the datasets (train, validation and tests) 
come from the same novel: \emph{I Promessi Sposi}. 

\subsection{Out-of-domain tests}

We designed experiments to test the same models on different authors.
For this purpose, we planned to use the dataset from \cite{redaelli-sprugnoli-2024-sentence}, in which different authors
from the same century are used.
The dataset has already the gold results annotated by the authors of the paper.

\section{Results}

\subsection{Embedding-based: in-domain}

\begin{table}[]
	\small
	\caption{Best F1-epoch Embedding-based}
	\begin{tabular}{llll}
		Models & Best Epoch & F1 & Valid loss \\
		BERT base cased & 6      & 0.9922        & 0.0014                  \\
		ModernBERT base ITA  & 12      & 0.9938        & 0.0097                  \\
		BERT base ITA XXL cased & 11      & 0.9922        & 0.0021                 \\
		XLM RoBERTa base & 4       & 0.9953        & 0.0014                  \\
		XLM RoBERTa large & 12       & 0.9953       & 0.0087                 \\
		Electra ITA XXL disc. & 20       & 0.9953        & 0.0015                
	\end{tabular}
	\label{t1}
\end{table}

Tables \ref{t1} reports the F1 scores and the losses on validation set 
for the best-F1 epoch, training the network with 30 epochs.
This is of course an in-domain evaluation, since the validation set
is from the same author (Manzoni) and even also the same novel
(I Promessi Sposi).
The scores look not so bad, probably we can expect they will diverge
more in case of out-of-domain evaluation.

\subsection{Embedding-based: out-of-domain}

\subsection{LLM-based: in-domain}

\subsection{LLM-based: out-of-domain}

\subsection{Embedding vs LLM: comparison}

\emph{This section is planned to compare the two approaches: embedding-based vs LLM-based.}
\emph{The comparison will be conducted once both implementations are fully executed and evaluated.}

The planned comparison metrics include:

 \begin{itemize}
	\item F1 score - in-domain (Manzoni dataset)
	\item F1 score - out-of-domain (other 19th century Italian authors)
	\item Memory usage during training and inference
	\item Training time requirements
	\item Inference speed
 \end{itemize}

\emph{Note: Actual results will be populated after running the experiments described in the notebooks.}

\appendix
\section{Additional details/describe Appendix}
\label{sec:appendix1}

\bibliography{custom}

\end{document}


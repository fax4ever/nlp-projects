\pdfoutput=1
\documentclass[11pt]{article}

\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{listings}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{%
	Sentence Splitter\\
	\large Multilingual Natural Language Processing \\
	Homework 2}
\date{August $10^{th}$, 2025}

\author{Ercoli Fabio Massimo \\
\texttt{802397} \\\And
Della Porta Nicolò \\
\texttt{1920468} \\}

\begin{document}

\maketitle

\section{Introduction}

Quoting \cite{redaelli-sprugnoli-2024-sentence} "Sentence splitting, that is the segmentation of the raw input text into sentences, is a fundamental step in text processing".  According to \cite{frohmann2024segmenttextuniversalapproach} the main challenges are:

\begin{itemize}
	\item Robustness to missing punctuation
	\item Effective adaptability to new domains
	\item High efficiency
\end{itemize}

According to \cite{redaelli-sprugnoli-2024-sentence} we can add to the list:

\begin{itemize}
	\item Multilinguality
\end{itemize}

Because a sentence splitting that works well for English may not work well to split another language.

In this project we implemented two types of models for sentence splitting, using an Italian corpus as train and validation set.
The first kind is based on an embedding model, the second is based on a generative LLM.
We aim to analyze and compare the two approaches. 
We also plan to test the models out of the original domain of training, that is an annotated text from I Promessi Sposi (Quarantana) by Manzoni.

\section{Methodology}

\subsection{Embedding-based}

We fine-tuned a pretrained embedding model using the train and validation datasets provided by the homework guide.

The original dataset provides two large texts (one to use as train and the other as validation) together with the golden labels to mark the end of sentence (1) and all the rest of the words (0).

In order to make the datasets suitable for training, we had to apply some transformations. 
We needed to group words into sequences, and to tokenize words into tokens, aligning the golden labels consistently.
With the constraint that the number of tokens of each sequence must fit the max length of the embedding models,
for instance 512.

We generated a dataset using sequences of 192 words each. Notice that the number of tokens for each sequence will be strictly greater,
since for each word the tokenizer of the model will produce one or more tokens.

Before using the datasets, we still need to align (as mentioned above) the labels to the tokens.
The alignment strategy we applied consists of keeping 1 as the first token generated from a word
having label 1, and using 0 for all the other cases.

One aspect that we had to address was the fact that the label distribution is very unbalanced
for this use case. Most of the labels are 0s, and few are 1s. We applied 2 different strategies.

First, we set the \textbf{load\_best\_model\_at\_end} training argument to \textbf{true},
using \textbf{metric\_for\_best\_model} set to F1. This implements early stopping based on F1,
rather than on accuracy (which would be misleading for such an unbalanced dataset).

Second, we overrode the loss function to weight misclassification of class 1 (end of sentence) 30× more than class 0. 
This weighting factor was motivated by the label distribution (96.7\% class 0 vs 3.3\% class 1). 
We implemented this using a custom weighted trainer. 

During inference, we don't need to align the labels, since the labels are generated by the model.

\subsection{LLM-based}

Fine-tuning an LLM is usually prohibitive or impossible using modest hardware, such as a single GPU with 16 GB / 24 GB RAM. So we applied two techniques to reduce the amount of memory required (and also the time) to fine-tune the models: LoRA (Low-Rank Adaptation) and parameter quantization.

A special focus was given to creating good prompts for both the training and inference phases.
We first generated new datasets from the ones we used for the embedding-based solution, in which 
for every sequence we have:

 \begin{itemize}
 	\item \emph{input\_text}: here we simply concatenate all words of the sequence into a single text
 	\item \emph{output\_text}: here we group all the words that belong to the same sentence on different lines, numbered sequentially
 \end{itemize}
 
 This is not a prompt yet. To create the final prompts, we created dictionaries of roles (system / user / assistant) with contents (we called them conversations) and passed them to the LLM tokenizer in order to have them converted into the proper chat templates. 
 
 We used the Supervised Fine-Tuning (TRL SFT) Trainer to train the model.

The fine-tuned models are (as usual) pushed to the Hugging Face repository, so that they can be used for inference from anywhere.

\section{Experiments}

\subsection{Embedding-based: Training Evaluation}

The performance goal we set is to maximize the F1 score on the validation set. Our expectation is that with a good result on the
validation set can generalize nicely on the test set.

For the embedding-based solution, we tested the following base models:

 \begin{itemize}
 	\item BERT base cased
	\item ModernBERT-base-ita (Dec 2024)
	\item Italian BERT XXL (most established)
	\item XLM-RoBERTa base
	\item XLM-RoBERTa large
	\item Italian ELECTRA
\end{itemize}

For the LLM-based solution, we tested the following base models:

 \begin{itemize}
 	\item Qwen3 4B
	\item Meta Llama 3.1 8B Instruct
	\item Mistral 7B Instruct v0.3 bnb 4bit
	\item Minerva 7B Instruct v1.0
\end{itemize}

As we said, both test and validation set come from the same novel: \emph{I Promessi Sposi}. 

\subsection{Embedding-based: Out of Domain Evaluation}

We designed experiments to test the same models on different authors.
For this purpose, we planned to use the dataset from \cite{redaelli-sprugnoli-2024-sentence}, in which different authors
from the same century are used.
The dataset has already the gold results annotated by the authors of the paper.

We evaluated the f1 score of the predictions, having the annotated
novels as ground true.

We produced labels from both the annotated novels and 
from predictions. For simplicity of the implementation,
for this phase, we produces a label for each character, instead of a label for each token.

The golden-annotated novel rows were grouped in chunk of equal size of lines, to produce sequences.

We tested the in-domain novel (Quarantana) and 3 novels (Pinocchio, I Malavoglia and Cuore) from different authors of the same period.

\subsection{LLM-based}

Results for LLM-based (generative)  models will be evaluated manually, simply taking an out of domain 
text fragment from Cuore novel and pass in input to the target test models.

\section{Results}

\subsection{Embedding-based: Training Evaluation}

\begin{table}[]
	\small
	\caption{Best F1-epoch Embedding-based}
	\begin{tabular}{llll}
		Models & Best Epoch & F1 & Valid loss \\
		BERT base cased & 6      & 0.9922        & 0.0014                  \\
		ModernBERT base ITA  & 12      & 0.9938        & 0.0097                  \\
		BERT base ITA XXL cased & 11      & 0.9922        & 0.0021                 \\
		XLM RoBERTa base & 4       & 0.9953        & 0.0014                  \\
		XLM RoBERTa large & 12       & 0.9953       & 0.0087                 \\
		Electra ITA XXL disc. & 20       & 0.9953        & 0.0015                
	\end{tabular}
	\label{t1}
\end{table}

Table \ref{t1} reports the F1 scores and the losses on validation set 
for the best-F1 epoch, training the network with 30 epochs.
This is of course an in-domain evaluation, since the validation set
is from the same author (Manzoni) and even also the same novel
(I Promessi Sposi).
On training, the F1s on the validation set look very good.

\subsection{Embedding-based: Out of Domain Evaluation}

\begin{table}[]
	\small
	\caption{Out-Of-Domain Evaluation: Embedding-based}
	\begin{tabular}{lllll}
		Models & Quara. & Pinocc. & Malav. & Cuore \\
		BERT base cased & 0.9922      & 0.8058 & 0.7304        & 0.8757                  \\
		ModernBERT ITA  & 0.9675     & 0.8495   & 0.8128   & 0.8864                  \\
		BERT ITA XXL  & 0.9938      & 0.9947   & 0.9205     & 0.9591                 \\
		XLM RoBERTa B & 0.3953       & 0.5427   & 0.1980     & 0.2424                 \\
		XLM RoBERTa L & 0.5275       & 0.6618    & 0.3559   & 0.5               \\
		Electra ITA XXL  & 0.9923       & 0.9785    & 0.9171    & 0.9655             
	\end{tabular}
	\label{t2}
\end{table}

When we evaluate the same models on different novels we got the
result reported in table \ref{t2}.

According to our tests, multilingual base models (XLM RoBERTa base and large)
got very bad results, even in the same domain (Quarantana).

It seems to be better to use BERT base, even if it does not trained on Italian.
This base model, like ModernBERT ITA, works very good in domain (Quarantana) and
decently out of domain (other novels).

BERT ITA XXL and Electra ITA XXL got great results both in domain and out of domain.

The novel that seems to be the more difficult to split for all model is I Malavoglia.
Best result is usually achived in domain (as expected).

\subsection{LLM-based}

\bibliography{custom}

\end{document}

